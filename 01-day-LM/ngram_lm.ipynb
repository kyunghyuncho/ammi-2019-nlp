{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ngram_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "## Goal: compute a probabilty distribution over all possible sentences:\n",
    "\n",
    "\n",
    "## $$p(W) = p(w_1, w_2, ..., w_T)$$\n",
    "\n",
    "## This unsupervised learning problem can be framed as a sequence of supervised learning problems:\n",
    "\n",
    "## $$p(W) = p(w_1) * p(w_2|w_1) * ... * p(w_T|w_1, ..., w_{T-1})$$\n",
    "\n",
    "## If we have N sentences, each of them with T words / tokens, then we want to max:\n",
    "\n",
    "## $$log p(W) = \\sum_{n = 1}^N \\sum_{i=1}^{T} log p(w_i | w_{<i})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram language model\n",
    "\n",
    "## Goal: estimate the n-gram probabilities using counts of sequences of n consecutive words\n",
    "\n",
    "## Given a sequence of words $w$, we want to compute\n",
    "\n",
    "##  $$P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1})$$\n",
    "\n",
    "## Where $w_i$ is the i-th word of the sequence.\n",
    "\n",
    "## $$P(w_i|w_{i−n+1}, ..., w_{i−2}, w_{i−1}) = \\frac{p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w_i)}{\\sum_{w \\in V} p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w)}$$\n",
    "\n",
    "## Key Idea: We can estimate the probabilities using counts of n-grams in our dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see this in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "#: implement the neural LM with concat instead of summation -- so that you have a fixed input etc.\n",
    "# make a separate\n",
    "# create some slides with pictures maybe explaining the model visualizations -- line by line\n",
    "# get google cloud working\n",
    "# make it work on gpu\n",
    "# show them kenlm and how to use to do different stuff with it\n",
    "# use the same sentences to generation and testing etc.\n",
    "# explain perplexity\n",
    "# ngram, ff, rnn, rnn+attention\n",
    "# do sentence generation\n",
    "# do long sentences\n",
    "# compare different n-grams -- 2,3,more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we install as needed and import as needed or all at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run if you dont have it installed\n",
    "# !pip install more_itertools\n",
    "# !pip install spacy# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\\\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCVSciOCAMZb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYs6AMs6AIre"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy\n",
    "import itertools\n",
    "from operator import itemgetter \n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import more_itertools as mit  # not built-in package\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext import vocab\n",
    "from collections import Counter\n",
    "import re\n",
    "from torchtext.data import TabularDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H20pktPiA63a",
    "outputId": "fb38d897-e889-4451-df77-9ca98eb266a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb38d099250>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext import vocab\n",
    "from collections import Counter\n",
    "import re\n",
    "from torchtext.data import TabularDataset \n",
    "\n",
    "class AmazonReviewsDataset(TabularDataset):\n",
    "    \n",
    "    urls = [\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz',\n",
    "           'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Tools_and_Home_Improvement_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Baby_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz',\n",
    "#            'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz',\n",
    "#             'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz',\n",
    "        ]\n",
    "    name='amazonreviews'\n",
    "    dirname='processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download_done = AmazonReviewsDataset.download(root='data/', check=True)\n",
    "\n",
    "# # if above does not work do this in stead in cd ammi-2019-nlp/data\n",
    "# !wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\n",
    "# !gunzip reviews_Clothing_Shoes_and_Jewelry_5.json.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETOK = re.compile(r'\\w+|[^\\w\\s]|\\n', re.UNICODE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return RETOK.findall(s)\n",
    "\n",
    "text_field = data.Field(sequential=True, tokenize=tokenize, include_lengths=True, use_vocab=True, lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmazonReviewsDataset(path='/home/roberta/ammi-2019-nlp/data/reviews_Clothing_Shoes_and_Jewelry_5.json', format='json', fields={'reviewText': ('reviewText', text_field), 'summary': ('summary', text_field)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples : 278677\n",
      "Review: \n",
      " ['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.', 'it', 'doesn', \"'\", 't', 'look', 'cheap', 'at', 'all', '.', 'i', \"'\", 'm', 'so', 'glad', 'i', 'looked', 'on', 'amazon', 'and', 'found', 'such', 'an', 'affordable', 'tutu', 'that', 'isn', \"'\", 't', 'made', 'poorly', '.', 'a', '+', '+'] \n",
      "\n",
      " Summary: \n",
      " ['great', 'tutu', '-', 'not', 'cheaply', 'made']\n"
     ]
    }
   ],
   "source": [
    "# lets check it\n",
    "# lets use fstrings btw\n",
    "print(f'Number of samples : {len(dataset.examples)}')\n",
    "\n",
    "for ex in dataset.examples:\n",
    "    print(f'Review: \\n {ex.reviewText} \\n\\n Summary: \\n {ex.summary}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278677"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dataset to a list of strings\n",
    "# each string represents a review\n",
    "all_reviews = []\n",
    "for ex in dataset.examples:\n",
    "    all_reviews.append(ex.reviewText)\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'tutu',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'really',\n",
       " 'great',\n",
       " 'price',\n",
       " '.',\n",
       " 'it',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'look',\n",
       " 'cheap',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'so',\n",
       " 'glad',\n",
       " 'i',\n",
       " 'looked',\n",
       " 'on',\n",
       " 'amazon',\n",
       " 'and',\n",
       " 'found',\n",
       " 'such',\n",
       " 'an',\n",
       " 'affordable',\n",
       " 'tutu',\n",
       " 'that',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'made',\n",
       " 'poorly',\n",
       " '.',\n",
       " 'a',\n",
       " '+',\n",
       " '+']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_tqdm = tqdm_notebook  # prolly you need jupyter widget for this, change for tqdm for simple tqdm\n",
    "\n",
    "NUM_SENTENCES = len(all_reviews)\n",
    "NUM_SENTENCES_TRAIN = int(0.8*NUM_SENTENCES)\n",
    "NUM_SENTENCES_TEST = int(0.1*NUM_SENTENCES)\n",
    "NUM_SENTENCES_VALID = NUM_SENTENCES - NUM_SENTENCES_TRAIN - NUM_SENTENCES_TEST\n",
    "\n",
    "train_reviews = all_reviews[:NUM_SENTENCES_TRAIN]\n",
    "test_reviews = all_reviews[NUM_SENTENCES_TRAIN:NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST]\n",
    "valid_reviews = all_reviews[NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST:NUM_SENTENCES_TRAIN+NUM_SENTENCES_TEST+NUM_SENTENCES_VALID]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 222941,\n",
       " list,\n",
       " str,\n",
       " '.',\n",
       " ['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'look',\n",
       "  'cheap',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'so',\n",
       "  'glad',\n",
       "  'i',\n",
       "  'looked',\n",
       "  'on',\n",
       "  'amazon',\n",
       "  'and',\n",
       "  'found',\n",
       "  'such',\n",
       "  'an',\n",
       "  'affordable',\n",
       "  'tutu',\n",
       "  'that',\n",
       "  'isn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'made',\n",
       "  'poorly',\n",
       "  '.',\n",
       "  'a',\n",
       "  '+',\n",
       "  '+'])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_reviews), len(train_reviews), type(train_reviews[0]), type(train_reviews[0][0]), train_reviews[0][11], train_reviews[0]#, test_reviews[0], valid_reviews[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .txt files with the reviews\n",
    "\n",
    "with open('../data/amazon_reviews_clothing_train.txt', 'w') as f:\n",
    "    for review in train_reviews:\n",
    "        for token in review:\n",
    "            f.write(\"%s \" % token) \n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "with open('../data/amazon_reviews_clothing_test.txt', 'w') as f:\n",
    "    for review in test_reviews:\n",
    "        for token in review:\n",
    "            f.write(\"%s \" % token) \n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "with open('../data/amazon_reviews_clothing_valid.txt', 'w') as f:\n",
    "    for review in valid_reviews:\n",
    "        for token in review:\n",
    "            f.write(\"%s \" % token) \n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     train_data.append(review.split())\n",
    "    \n",
    "test_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_test.txt', 'r') as f:\n",
    "    test_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     test_data.append(review.split())\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     valid_data.append(review.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 222919, str, 184, str, 1)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(train_data), len(train_data), \\\n",
    "type(train_data[0]), len(train_data[0]), \\\n",
    "type(train_data[0][0]), len(train_data[0][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " 't')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], train_data[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')               \n",
    "punctuations = string.punctuation\n",
    "# punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~' \n",
    "TAG_RE = re.compile(r'<[^>]+>') # get rid off HTML tags from the data\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def lower_case(parsed):\n",
    "    return [token.text.lower() for token in parsed] #and (token.is_stop is False)]\n",
    "\n",
    "def remove_punc(parsed):\n",
    "    return [token.text for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)] #and (token.is_stop is False)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "   # tokenize each sentence -- each tokenized sentence will be an element in token_dataset\n",
    "    token_dataset = []\n",
    "    # tokenize all words -- each token will be an item in all_tokens (in the order given by the list of sentences)\n",
    "    all_tokens = []     # all the tokens -- \n",
    "\n",
    "    for sample in _tqdm(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "#         tokens = lower_case_remove_punc(sample)\n",
    "        tokens = lower_case(sample)       # make words lower case\n",
    "#         tokens = remove_punct(tokens)     # remove punctuation\n",
    "        token_dataset.append(tokens)    \n",
    "        all_tokens += tokens\n",
    "        \n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', '!', str, 32, str)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations, punctuations[0], \\\n",
    "type(punctuations), len(punctuations), type(punctuations[0]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<[^>]+>', re.UNICODE)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for now only work with small subset of the data -- switch to all data later\n",
    "train_data = train_data[:80]\n",
    "test_data = test_data[:10]\n",
    "valid_data = valid_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, str, str)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data), type(train_data[0]), type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99065da68684fa9b3d8573444e47395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f57cf1c1a64057a1630b7ff284978a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e68e209d114ab4a628a16cb0573cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = tokenize_dataset(train_data)\n",
    "test_data_tokenized, all_tokens_test = tokenize_dataset(test_data)\n",
    "valid_data_tokenized, all_tokens_valid = tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokenized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14771,\n",
       " 'this',\n",
       " 80,\n",
       " ['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'look',\n",
       "  'cheap',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'so',\n",
       "  'glad',\n",
       "  'i',\n",
       "  'looked',\n",
       "  'on',\n",
       "  'amazon',\n",
       "  'and',\n",
       "  'found',\n",
       "  'such',\n",
       "  'an',\n",
       "  'affordable',\n",
       "  'tutu',\n",
       "  'that',\n",
       "  'isn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'made',\n",
       "  'poorly',\n",
       "  '.',\n",
       "  'a',\n",
       "  '+',\n",
       "  '+'])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of All Tokens\n",
    "len(all_tokens_train), all_tokens_train[0], \\\n",
    "len(train_data_tokenized), train_data_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Vocabulary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 2091 words\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary using all the tokens found in train data (90% of most common ones)\n",
    "voc = list(set(all_tokens_train))\n",
    "print('Word vocabulary size: {} words'.format(len(voc)))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS ANALYSIS (Train + Valid Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Tokens in the Corpus Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All Tokens  14771\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All Tokens \", len(all_tokens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All UNIQUE Tokens  2091\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All UNIQUE Tokens \", len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Sentences in the Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences  80\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Sentences \", len(train_data_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count how often each sentence length occurs. Visualize this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change n to see how the results change\n",
    "#### unigrams: n = 1; bigrams: n = 2; trigrams: n = 3;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of words and their corresponding frequencies. Which are the 10 most frequent words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for padding the sentences with special markers sentence beginning and end, i.e. $<bos>$ and $<eos>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        padded = [\"<bos>\" for i in range((n - 1))] + l +[\"<eos>\" for i in range((n - 1))]\n",
    "        result_list.append(padded)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sentences(train_data_tokenized, n)\n",
    "valid_padded = pad_sentences(valid_data_tokenized, n)\n",
    "test_padded = pad_sentences(test_data_tokenized, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_padded[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for finding all N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append(list(zip(*[l[i:] for i in range(n)])))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the dataset to its corresponding n-gram version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ngram[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram counts for already tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counts(data, n, frac_vocab=0.9):    \n",
    "    all_train_tokens = list(mit.flatten(data))\n",
    "    counted_tokens = Counter(all_train_tokens)\n",
    "    max_vocab_size = int(frac_vocab * len(counted_tokens))\n",
    "\n",
    "    vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "    \n",
    "    return vocab, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ngram, count_ngram = ngram_counts(train_ngram, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngram = find_ngrams(train_padded, n)\n",
    "vocab_trigram, count_trigram = ngram_counts(train_ngram, n)\n",
    "vocab_trigram, count_trigram = ngram_counts(train_ngram, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_ngram[:3], count_ngram[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_dict(vocab):\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(4, 4+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>', '<bos>', '<eos>'] + id2token\n",
    "\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    token2id['<bos>'] = BOS_IDX \n",
    "    token2id['<eos>'] = EOS_IDX\n",
    "\n",
    "    return id2token, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token_ngram, token2id_ngram = ngram_dict(vocab_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2token_ngram[:10], \\\n",
    "# token2id_ngram['<unk>'], token2id_ngram['<eos>'], token2id_ngram[('rosetta', 'stone')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 11204 ; token ('or', 'two', 'before')\n",
      "Token ('or', 'two', 'before'); token id 11204\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token_ngram) - 1)\n",
    "random_token = id2token_ngram[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_ngram[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_ngram[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text2id(doc, token2id):\n",
    "    return [token2id[t] if t in token2id else UNK_IDX for t in doc]\n",
    "\n",
    "def _id2text(vec, id2token):\n",
    "    return [id2token[i] for i in vec]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_id(data, token2id):\n",
    "    data_id = []\n",
    "    for d in data:\n",
    "        data_id.append(_text2id(d, token2id))\n",
    "    return data_id\n",
    "\n",
    "def create_data_id_merged(data, token2id):\n",
    "    data_id_merged = []\n",
    "    for d in data:\n",
    "        data_id_merged.append((d, 0))\n",
    "    return data_id_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = create_data_id(train_data_ngram, token2id_ngram)\n",
    "data_id_merged = create_data_id_merged(data_id, token2id_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data_id), data_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data_id_merged), data_id_merged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that combines all the aboce and goes from tokenized data to the ngram dataset\n",
    "def create_id_dataset(data, n):\n",
    "    padded_data = pad_sentences(data, n)\n",
    "    ngram_data = find_ngrams(padded_data, n)\n",
    "    \n",
    "    vocab, count = ngram_counts(ngram_data, n)    \n",
    "    id2token, token2id = ngram_dict(vocab)\n",
    "    \n",
    "    data_id = create_data_id(ngram_data, token2id)\n",
    "    data_id_merged = create_data_id_merged(data_id, token2id)\n",
    "    \n",
    "    return data_id, data_id_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_id, all_data_id_merged = create_id_dataset(train_data_tokenized, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data_id[0], all_data_id_merged[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the probability of an n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('.', '<eos>', '<eos>'),\n",
       "  ('it', \"'\", 's'),\n",
       "  ('.', '.', '.'),\n",
       "  ('i', \"'\", 'm'),\n",
       "  ('<bos>', '<bos>', 'i'),\n",
       "  ('don', \"'\", 't'),\n",
       "  ('!', '<eos>', '<eos>'),\n",
       "  ('i', \"'\", 've'),\n",
       "  ('.', 'i', \"'\"),\n",
       "  ('you', \"'\", 're')),\n",
       " (59, 40, 33, 26, 23, 23, 14, 14, 13, 13))"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ngram[:10], count_ngram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_count(ngram, vocab, count):\n",
    "    if ngram in vocab:\n",
    "        ngram_idx = vocab.index(ngram)\n",
    "        return count[ngram_idx] \n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_ngram_count(('.', 'i'), vocab_ngram, count_ngram)\n",
    "c\n",
    "\n",
    "# c = get_ngram_count(('baby', 'panda'), vocab_ngram, count_ngram)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_ngram_count(('it', \"'\", 's'), vocab_ngram, count_ngram)\n",
    "c\n",
    "\n",
    "\n",
    "# c = get_ngram_count(('baby', 'panda', 'sweet'), vocab_ngram, count_ngram)\n",
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the probability of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob(ngram, vocab, count):\n",
    "    c = get_ngram_count(ngram, vocab, count)\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "            print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.8846153846153846)"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob(('rosetta', 'stone'), vocab_ngram, count_ngram)\n",
    "p, 69/(69+2+2+1+1+1+1+1)\n",
    "\n",
    "# p = get_ngram_prob(('i', 'am'), vocab_ngram, count_ngram)\n",
    "# p\n",
    "\n",
    "# p = get_ngram_prob(('it', \"'\", 's'), vocab_ngram, count_ngram)\n",
    "# p\n",
    "\n",
    "# p = get_ngram_prob(('i', \"like\", 'it'), vocab_ngram, count_ngram)\n",
    "# p, 1/(2+1+1+1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_sentence(sentence, vocab, count):\n",
    "    padded_sentence = pad_sentences(sentence, n)  # needs a list\n",
    "#     print(padded_sentence)\n",
    "    ngram_sentence = find_ngrams(padded_sentence, n)[0] # only one element in list\n",
    "#     print(ngram_sentence)\n",
    "    prob = 1\n",
    "    for ngram in ngram_sentence:\n",
    "        prob_ngram = get_ngram_prob(ngram, vocab, count)\n",
    "        print(ngram, prob_ngram)\n",
    "        prob *= prob_ngram\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.', 'it', 'doesn', \"'\", 't', 'look', 'cheap', 'at', 'all', '.', 'i', \"'\", 'm', 'so', 'glad', 'i', 'looked', 'on', 'amazon', 'and', 'found', 'such', 'an', 'affordable', 'tutu', 'that', 'isn', \"'\", 't', 'made', 'poorly', '.', 'a', '+', '+']]\n",
      "('<bos>', '<bos>', 'this') 0.0684931506849315\n",
      "('<bos>', 'this', 'is') 0.5\n",
      "('this', 'is', 'a') 0.45\n",
      "('is', 'a', 'great') 0.16\n",
      "('a', 'great', 'tutu') 0.09090909090909091\n",
      "('great', 'tutu', 'and') 0.5\n",
      "('tutu', 'and', 'at') 1.0\n",
      "('and', 'at', 'a') 0.3333333333333333\n",
      "('at', 'a', 'really') 0.16666666666666666\n",
      "('a', 'really', 'great') 1.0\n",
      "('really', 'great', 'price') 1.0\n",
      "('great', 'price', '.') 1.0\n",
      "('price', '.', 'it') 0.4\n",
      "('.', 'it', 'doesn') 0.022222222222222223\n",
      "('it', 'doesn', \"'\") 1.0\n",
      "('doesn', \"'\", 't') 1.0\n",
      "(\"'\", 't', 'look') 0.016129032258064516\n",
      "('t', 'look', 'cheap') 1.0\n",
      "('look', 'cheap', 'at') 1.0\n",
      "('cheap', 'at', 'all') 1.0\n",
      "('at', 'all', '.') 0.42857142857142855\n",
      "('all', '.', 'i') 0.5\n",
      "('.', 'i', \"'\") 0.11504424778761062\n",
      "('i', \"'\", 'm') 0.5416666666666666\n",
      "(\"'\", 'm', 'so') 0.08\n",
      "('m', 'so', 'glad') 0.5\n",
      "('so', 'glad', 'i') 1.0\n",
      "('glad', 'i', 'looked') 1.0\n",
      "('i', 'looked', 'on') 1.0\n",
      "('looked', 'on', 'amazon') 1.0\n",
      "('on', 'amazon', 'and') 0.5\n",
      "('amazon', 'and', 'found') 1.0\n",
      "('and', 'found', 'such') 0.25\n",
      "('found', 'such', 'an') 1.0\n",
      "('such', 'an', 'affordable') 0.5\n",
      "('an', 'affordable', 'tutu') 1.0\n",
      "('affordable', 'tutu', 'that') 1.0\n",
      "('tutu', 'that', 'isn') 1.0\n",
      "('that', 'isn', \"'\") 1.0\n",
      "('isn', \"'\", 't') 1.0\n",
      "(\"'\", 't', 'made') 0.016129032258064516\n",
      "('t', 'made', 'poorly') 1.0\n",
      "('made', 'poorly', '.') 1.0\n",
      "('poorly', '.', 'a') 1.0\n",
      "('.', 'a', '+') 0.5\n",
      "('a', '+', '+') 1.0\n",
      "('+', '+', '<eos>') 0.16666666666666666\n",
      "('+', '<eos>', '<eos>') 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0055998183675866e-17"
      ]
     },
     "execution_count": 1247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [train_data_tokenized[0]]\n",
    "# sentence = [['this', 'is', 'a', 'great', 'tutu']]\n",
    "print(sentence)\n",
    "ps = get_prob_sentence(sentence, vocab_ngram, count_ngram)\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=False):\n",
    "    pd = [0 for v in voc]\n",
    "    for idx, token in enumerate(voc):\n",
    "#         print(\"token: \", token)\n",
    "#         print(\"prev ngram: \", prev_tokens)\n",
    "#         print(\"both: \", tuple(list(prev_tokens) + [token]))\n",
    "#         print(\"\")\n",
    "        token_ngram = tuple(list(prev_tokens) + [token])\n",
    "        pd[idx] = get_ngram_prob(token_ngram, vocab_ngram, count_ngram)\n",
    "        if pd[idx] > 0 and print_nonzero_probs:\n",
    "            print(token_ngram, \" \", pd[idx])\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'\", 'm')\n",
      "(\"'\", 'm', 'finding')   0.04\n",
      "(\"'\", 'm', 'enrolled')   0.04\n",
      "(\"'\", 'm', 'one')   0.04\n",
      "(\"'\", 'm', 'fluent')   0.04\n",
      "(\"'\", 'm', 'very')   0.04\n",
      "(\"'\", 'm', 'enjoying')   0.04\n",
      "(\"'\", 'm', 'a')   0.04\n",
      "(\"'\", 'm', 'given')   0.04\n",
      "(\"'\", 'm', 'not')   0.32\n",
      "(\"'\", 'm', 'hearing')   0.04\n",
      "(\"'\", 'm', 'sure')   0.08\n",
      "(\"'\", 'm', 'most')   0.04\n",
      "(\"'\", 'm', 'happy')   0.04\n",
      "(\"'\", 'm', 'so')   0.08\n",
      "(\"'\", 'm', 'calling')   0.04\n",
      "(\"'\", 'm', 'more')   0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prob distr for the word following prev_tokens (i.e. tutu) \n",
    "# over all the words in the vocabulary \n",
    "\n",
    "# prev_tokens = train_data_tokenized[0][4] #[0]\n",
    "prev_tokens = vocab_ngram[3][1:] #[0]   # need frmo 1 on so that this is a correct prev token\n",
    "print(prev_tokens)\n",
    "pd = get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=True)\n",
    "sum(pd)#, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=False):\n",
    "    pd = get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=print_nonzero_probs)\n",
    "    idx_next_token = np.random.choice(len(voc), 1, p=pd)[0]\n",
    "    return voc[idx_next_token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'\", 'm')\n",
      "(\"'\", 'm', 'finding')   0.04\n",
      "(\"'\", 'm', 'enrolled')   0.04\n",
      "(\"'\", 'm', 'one')   0.04\n",
      "(\"'\", 'm', 'fluent')   0.04\n",
      "(\"'\", 'm', 'very')   0.04\n",
      "(\"'\", 'm', 'enjoying')   0.04\n",
      "(\"'\", 'm', 'a')   0.04\n",
      "(\"'\", 'm', 'given')   0.04\n",
      "(\"'\", 'm', 'not')   0.32\n",
      "(\"'\", 'm', 'hearing')   0.04\n",
      "(\"'\", 'm', 'sure')   0.08\n",
      "(\"'\", 'm', 'most')   0.04\n",
      "(\"'\", 'm', 'happy')   0.04\n",
      "(\"'\", 'm', 'so')   0.08\n",
      "(\"'\", 'm', 'calling')   0.04\n",
      "(\"'\", 'm', 'more')   0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'enjoying'"
      ]
     },
     "execution_count": 1257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prev_tokens)\n",
    "next_token = sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=True)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(num_tokens, vocab_ngram, count_ngram, voc, n):\n",
    "    sentence = []\n",
    "    prev_tokens = tuple(['<bos>'] * (n - 1))\n",
    "#     print(prev_tokens)\n",
    "    for i in range(num_tokens):\n",
    "        next_token = sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc)\n",
    "#         print(i, next_token)\n",
    "#         print(i, prev_tokens[1:])\n",
    "        prev_tokens = tuple(list(prev_tokens[1:]) + [next_token])\n",
    "#         print(i, prev_tokens)\n",
    "        sentence.append(next_token)\n",
    "        print(' '.join(sentence))\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "i bought\n",
      "i bought this\n",
      "i bought this for\n",
      "i bought this for christmas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i bought this for christmas'"
      ]
     },
     "execution_count": 1269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = 5\n",
    "generated_sentence = generate_sentence(num_tokens, vocab_ngram, count_ngram, voc, n)\n",
    "generated_sentence\n",
    "\n",
    "# TODO: make this owkr for general ngram -- double check that it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-One Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_add_one_smoothing(ngram, vocab, count):\n",
    "    c = get_ngram_count(ngram, vocab, count) + 1\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "            print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    all_counts += len(voc)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: show examples\n",
    "p = get_ngram_prob(('am', 'rosetta', 'stone'), vocab_ngram, count_ngram)\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00047824007651841227"
      ]
     },
     "execution_count": 1347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_add_one_smoothing(('am', 'rosetta', 'stone'), vocab_ngram, count_ngram)\n",
    "p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_addditive_smoothing(ngram, vocab, count, delta=0.5):\n",
    "    c = get_ngram_count(ngram, vocab, count) + delta*1\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "            print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    all_counts += delta*len(voc)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004782400765184122"
      ]
     },
     "execution_count": 1345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_addditive_smoothing(('am', 'rosetta', 'stone'), vocab_ngram, count_ngram, delta=0.1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Interpolation Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_interpolation_smoothing(ngram, vocab, count, prev_vocab, prev_count, alpha=0.5):\n",
    "    c = get_ngram_count(ngram, vocab, count)\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "            print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    if all_counts > 0:\n",
    "        prob_ngram = c / all_counts\n",
    "    else:\n",
    "        prob_ngram = 0\n",
    "    \n",
    "    prev_ngram = tuple(list(ngram[1:]))\n",
    "    prev_c = get_ngram_count(prev_ngram, prev_vocab, prev_count)\n",
    "    print(prev_c)\n",
    "    prev_all_counts = 0\n",
    "    for prev_t in prev_vocab:\n",
    "        if prev_t[:-1] == prev_ngram[:-1]:\n",
    "            print(prev_t, get_ngram_count(prev_t, prev_vocab, prev_count))\n",
    "            prev_all_counts += get_ngram_count(prev_t, prev_vocab, prev_count)\n",
    "    if prev_all_counts > 0:\n",
    "        prob_prev_ngram = prev_c / prev_all_counts\n",
    "    else:\n",
    "        0\n",
    "    return alpha*(prob_ngram) + (1-alpha)*prob_prev_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sentences(train_data_tokenized, 3)\n",
    "train_trigram = find_ngrams(train_padded, 3)\n",
    "vocab_trigram, count_trigram = ngram_counts(train_trigram, 3)\n",
    "\n",
    "\n",
    "train_padded = pad_sentences(train_data_tokenized, 2)\n",
    "train_bigram = find_ngrams(train_padded, 2)\n",
    "vocab_bigram, count_bigram = ngram_counts(train_bigram, 2)\n",
    "\n",
    "train_padded = pad_sentences(train_data_tokenized, 1)\n",
    "train_unigram = find_ngrams(train_padded, 1)\n",
    "vocab_unigram, count_unigram = ngram_counts(train_unigram, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "('rosetta', 'stone') 69\n",
      "('rosetta', 'studio') 2\n",
      "('rosetta', 'world') 2\n",
      "('rosetta', ',') 1\n",
      "('rosetta', 'very') 1\n",
      "('rosetta', 'level') 1\n",
      "('rosetta', 'program') 1\n",
      "('rosetta', 'stones') 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4423076923076923"
      ]
     },
     "execution_count": 1344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_interpolation_smoothing(('am', 'rosetta', 'stone'), \\\n",
    "    vocab_trigram, count_trigram, vocab_bigram, count_bigram, alpha=0.5)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing: Linear Interpolation with Absolute Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_interpolation_discounting_smoothing(ngram, vocab, count, prev_vocab, prev_count, alpha=0.5):\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$p_{bi}(w|v) = max \\large{ \\frac{N(v, w) - b_{bi}}{N(v), 0}  + b_{bi} \\frac{V - N_0(v, \\cdot)}{N(v)} p_{uni}(w) \\large}$$\n",
    "\n",
    "### $$p_{uni}(w) = max \\large{ \\frac{N(w) - b_{uni}}{N, 0}  + b_{uni} \\frac{V - N_0(\\cdot)}{N} \\frac{1}{V}}$$\n",
    "\n",
    "### $$b_{bi} = \\frac{N_1(\\cdot, \\cdot)}{N_1(\\cdot, \\cdot) + 2*N_2(\\cdot, \\cdot)}$$\n",
    "\n",
    "### $$b_{uni} = \\frac{N_1(\\cdot)}{N_1(\\cdot) + 2*N_2(\\cdot)}$$\n",
    "\n",
    "\n",
    "### $$N_r(\\cdot) = \\sum_{w: N(w) = r} 1$$\n",
    "\n",
    "### $$N_r(\\cdot, \\cdot) = \\sum_{v, w: N(v, w) = r} 1$$\n",
    "\n",
    "### $$N_r(v, \\cdot) = \\sum_{w: N(v, w) = r} 1$$\n",
    "\n",
    "### V is the number of words in the vocabulary\n",
    "\n",
    "### $N_r(\\cdot, \\cdot)$ and $N_r(\\cdot)$  are the count-counts for bigrams and unigrams respectively $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's compute the bigram frequencies / probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that the probabilities sum up to one\n",
    "### $$\\sum_w p_{bi}(w|v) = \\sum_w p_{uni}(w) = 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the sums\n",
    "\n",
    "# show rank for each word in a sentence\n",
    "# explain perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some examples and see if they make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $PP = exp(-\\frac{LL}{\\sum_k(N_k + 1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $LL = \\sum_{k=1}^{K} \\sum_{n=1}^{N_k + 1} log p_{bi}(w_{k,n} | w_{k,n-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a complete list of trigrams occurring in the corpus. Which are the 10 most frequent trigrams?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine count statistics on the trigram frequencies, i.e. compute so-called count-counts (how many trigrams occur once, twice, : : :). Plot their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of trigrams in the corpus using only the words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate count statistics of the trigram frequencies for this modified corpus as well. What do you notice in comparison to the previous exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the out-of-vocabulary (OOV) rate, i.e. the percentage of running words in the corpus which are not covered by the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Converting from Token to ID and back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1 you generated a trigram frequency list for a given vocabulary.\n",
    "Determine the list of bigram frequencies from it by summing over the first word position:\n",
    "N(v;w) = N(\u0001; v;w) =\n",
    "X\n",
    "u\n",
    "N(u; v;w)\n",
    "Analogously, recompute the frequencies of unigrams.\n",
    "Now, extract bigrams/unigrams directly from the corpus using your implementation from part 1\n",
    "and compare it to the recomputed versions. What do you notice? How could you \f",
    "x this problem\n",
    "without changing the recomputation method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the list of bigram frequencies from it by summing over the first word position:\n",
    "\n",
    "#### $ N(u, v, w) = \\sum_u N(\\cdot, v, w) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same for the unigrams. Do we get the same numbers as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(w|w_{−n}, ..., w_{−2}, w_{−1}) \\approx \\frac{c(w_{−n}, ..., w_{−2}, w_{−1}, w)}{\\sum_{w \\in V} c(w_{−n}, ..., w_{−2}, w_{−1}, w)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Probabilities\n",
    "\n",
    "## $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{\\sum_{w_i} c(w_{i-1}, w_i)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "##  $$p(s) = \\prod_{i = 1} ^ {N + 1} p(w_i | w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "### Bigram LM: $$ p(i \\; love \\; this \\; light) = p(i|\\cdot) \\; p(love|i)\\;  p(this|love)\\;  p(light|this) \\\\\n",
    "\\approx \\frac{c(i, \\cdot)}{\\sum_w c(\\cdot, \\; w)} \\; \\frac{c(love, i)}{\\sum_wc(i, \\; w)}\\;  \\frac{c(this, love)}{\\sum_wc(love, \\;w)}\\;  \\frac{c(light, this)}{\\sum_wc(this, \\;w)}$$ \n",
    "\n",
    "### Trigram LM: $$ p(i \\; love \\; this  \\;light) = p(i|\\cdot, \\cdot) \\; p(love|\\cdot, i) \\; p(this|i, love)\\;  p(light|love, this)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example -- where this approach usually fails\n",
    "\n",
    "### Bigram LM: $$ p(john \\; went \\; to \\; the \\; moon) = p(john|\\cdot) p(went|john) p(to|went) p(the|to) p(moon|the)$$ \n",
    "\n",
    "### Trigram LM: $$ pp(john \\; went \\; to \\; the \\; moon = p(john|\\cdot, \\cdot) p(went|\\cdot, john) p(to|john, went) p(the|went, to) p(moon|to, the)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing -- which ones to show? comparisons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need smoothing for n-gram language modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentece Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a PyTorch Dataset out of our set of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        for (i, t) in tqdm_notebook(self.data):\n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]), torch.LongTensor([t])))\n",
    "            #TODO: fix error regarding to(device)\n",
    "#             self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), torch.LongTensor([t]).to(device)))\n",
    "              \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        \n",
    "        return inp, tgt, len(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-1]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1464fa77da84420aa1bdc036b848e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88572851c444b09aa30b75c176ebb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImdbDataset(train_data_id_merged, max_inp_length=None, device='cuda')\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=batchify, shuffle=True)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data_id_merged, max_inp_length=None, device='cuda')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ATBQ4WaJR93"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.init' has no attribute 'xavier_uniform_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-8a5333620466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBagOfNGrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-8a5333620466>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, emb_dim, hidden_size, reduce, nlayers, act, nclasses, dropout, batch_norm)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-8a5333620466>\u001b[0m in \u001b[0;36minit_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.init' has no attribute 'xavier_uniform_'"
     ]
    }
   ],
   "source": [
    "class BagOfNGrams(nn.Module):\n",
    "    def init_layers(self):\n",
    "        for l in self.layers:\n",
    "            if getattr(l, 'weight', None) is not None:\n",
    "                torch.nn.init.xavier_uniform_(l.weight)\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim=300, hidden_size=512, reduce='sum', nlayers=2, act='ReLU', nclasses=2, dropout=0.1, batch_norm=False):\n",
    "        super(BagOfNGrams, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.reduce = reduce\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.act = getattr(nn, act)\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_dim, mode=reduce)\n",
    "        if batch_norm is True:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.emb_dim)\n",
    "        self.layers = nn.ModuleList([nn.Linear(self.emb_dim, self.hidden_size)])\n",
    "        self.layers.append(self.act())\n",
    "#         self.layers.append(nn.Dropout(p=dropout))\n",
    "        for i in range(self.nlayers-2):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(self.act())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(self.hidden_size, 1))\n",
    "        self.init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        postemb = self.embedding(x)\n",
    "        if hasattr(self, 'batch_norm'):\n",
    "            x = self.batch_norm(postemb)\n",
    "        else:\n",
    "            x = postemb\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BagOfNGrams(len(id2token), emb_dim=30, hidden_size=2048, act='Tanh', nlayers=1, reduce='mean', dropout=0.0, batch_norm=False)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-a96dd0cf1945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, nesterov=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#optimizer = torch.optim.Adagrad(params=model.parameters(), lr=learning_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, nesterov=True)\n",
    "#optimizer = torch.optim.Adagrad(params=model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in loader:\n",
    "        outputs = torch.sigmoid(model(data))\n",
    "        predicted = (outputs > 0.5).long()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxfi7ruGHZu3"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-b8b8801c9c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO fix cuda issue!!! \n",
    "train_losses = []\n",
    "num_epochs=20\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in tqdm(enumerate(train_loader)): \n",
    "        data.cpu()\n",
    "        labels.cpu()\n",
    "        model.cpu()\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs.view(-1), labels.float().view(-1))\n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    if epoch % 1 == 0 and epoch > 0:\n",
    "        val_acc = test_model(loader=valid_loader, model=model)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Train loss: {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss.item(), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roberta/miniconda3/lib/python3.6/site-packages/altair/utils/core.py:90: UserWarning: I don't know how to infer vegalite type from 'empty'.  Defaulting to nominal.\n",
      "  \"Defaulting to nominal.\".format(typ))\n"
     ]
    },
    {
     "data": {
      "application/vnd.vegalite.v2+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v2.6.0.json",
       "config": {
        "view": {
         "height": 300,
         "width": 400
        }
       },
       "data": {
        "name": "data-d751713988987e9331980363e24189ce"
       },
       "datasets": {
        "data-d751713988987e9331980363e24189ce": []
       },
       "encoding": {
        "x": {
         "field": "step",
         "scale": {},
         "type": "nominal"
        },
        "y": {
         "field": "loss",
         "scale": {
          "type": "log"
         },
         "type": "nominal"
        }
       },
       "height": 400,
       "mark": "line",
       "width": 800
      },
      "image/png": "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",
      "text/plain": [
       "<VegaLite 2 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install altair\n",
    "import altair as alt\n",
    "import pandas\n",
    "\n",
    "batch_loss = []\n",
    "for i,l in enumerate(train_losses):\n",
    "    batch_loss.append((i,l))\n",
    "\n",
    "df = pandas.DataFrame(batch_loss, columns=['step', 'loss'])\n",
    "\n",
    "alt.Chart(df).mark_line().encode(\n",
    "    alt.X('step', scale=alt.Scale()),\n",
    "    alt.Y('loss', scale=alt.Scale(type='log'))).properties(\n",
    "        width=800,\n",
    "        height=400\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhi0Ww8JKEkk"
   },
   "source": [
    "## Analysis & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare N-gram LM to Neural LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec:  19%|█▉        | 1.25G/6.60G [11:49<50:35, 1.76MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3abfa44c94ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.vector_cache'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    261\u001b[0m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting vectors into {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = vocab.FastText(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset, max_size=30000, vectors=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a batch iterator\n",
    "train_loader = data.BucketIterator(dataset=dataset, batch_size=4, sort_key=lambda x: len(x.reviewText), device=torch.device('cpu'), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vec2txt(vec):\n",
    "    return [text_field.vocab.itos[t] for t in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.reviewText[0][0])\n",
    "print(_vec2txt(batch.reviewText[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jzyCRhtBNit"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFDnznsFJRMI"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ngram-lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
