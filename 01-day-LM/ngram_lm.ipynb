{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ngram_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "## Goal: compute a probabilty distribution over all possible sentences:\n",
    "\n",
    "\n",
    "## $$p(W) = p(w_1, w_2, ..., w_T)$$\n",
    "\n",
    "## This unsupervised learning problem can be framed as a sequence of supervised learning problems:\n",
    "\n",
    "## $$p(W) = p(w_1) * p(w_2|w_1) * ... * p(w_T|w_1, ..., w_{T-1})$$\n",
    "\n",
    "## If we have N sentences, each of them with T words / tokens, then we want to max:\n",
    "\n",
    "## $$log p(W) = \\sum_{n = 1}^N \\sum_{i=1}^{T} log p(w_i | w_{<i})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram language model\n",
    "\n",
    "## Goal: estimate the n-gram probabilities using counts of sequences of n consecutive words\n",
    "\n",
    "## Given a sequence of words $w$, we want to compute\n",
    "\n",
    "##  $$P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1})$$\n",
    "\n",
    "## Where $w_i$ is the i-th word of the sequence.\n",
    "\n",
    "## $$P(w_i|w_{i−n+1}, ..., w_{i−2}, w_{i−1}) = \\frac{p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w_i)}{\\sum_{w \\in V} p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w)}$$\n",
    "\n",
    "## Key Idea: We can estimate the probabilities using counts of n-grams in our dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see this in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "#: implement the neural LM with concat instead of summation -- so that you have a fixed input etc.\n",
    "# make a separate\n",
    "# create some slides with pictures maybe explaining the model visualizations -- line by line\n",
    "# get google cloud working\n",
    "# make it work on gpu\n",
    "# show them kenlm and how to use to do different stuff with it\n",
    "# use the same sentences to generation and testing etc.\n",
    "# explain perplexity\n",
    "# ngram, ff, rnn, rnn+attention\n",
    "# do sentence generation\n",
    "# do long sentences\n",
    "# compare different n-grams -- 2,3,more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we install as needed and import as needed or all at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run if you dont have it installed\n",
    "# !pip install more_itertools\n",
    "# !pip install spacy# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\\\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, \"/home/roberta/ParlAI\")\n",
    "# print(sys.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCVSciOCAMZb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYs6AMs6AIre"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy\n",
    "import itertools\n",
    "from operator import itemgetter \n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "_tqdm = tqdm_notebook\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import more_itertools as mit  # not built-in package\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext import vocab\n",
    "from collections import Counter\n",
    "import re\n",
    "from torchtext.data import TabularDataset \n",
    "import pandas\n",
    "import altair\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H20pktPiA63a",
    "outputId": "fb38d897-e889-4451-df77-9ca98eb266a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdf7c7e6930>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create .txt files with the reviews\n",
    "\n",
    "# with open('../data/amazon_reviews_clothing_train.txt', 'w') as f:\n",
    "#     for review in train_reviews:\n",
    "#         for token in review:\n",
    "#             f.write(\"%s \" % token) \n",
    "#         f.write(\"\\n\")\n",
    "        \n",
    "# with open('../data/amazon_reviews_clothing_test.txt', 'w') as f:\n",
    "#     for review in test_reviews:\n",
    "#         for token in review:\n",
    "#             f.write(\"%s \" % token) \n",
    "#         f.write(\"\\n\")\n",
    "        \n",
    "# with open('../data/amazon_reviews_clothing_valid.txt', 'w') as f:\n",
    "#     for review in valid_reviews:\n",
    "#         for token in review:\n",
    "#             f.write(\"%s \" % token) \n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     train_data.append(review.split())\n",
    "    \n",
    "test_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_test.txt', 'r') as f:\n",
    "    test_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     test_data.append(review.split())\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "# split each review into the tokens that compose it\n",
    "# for review in reviews:\n",
    "#     valid_data.append(review.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 222919, str, 184, str, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data), len(train_data), \\\n",
    "type(train_data[0]), len(train_data[0]), \\\n",
    "type(train_data[0][0]), len(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " 't')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], train_data[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')               \n",
    "punctuations = string.punctuation\n",
    "# punctuations = '\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~' \n",
    "TAG_RE = re.compile(r'<[^>]+>') # get rid off HTML tags from the data\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def lower_case(parsed):\n",
    "    return [token.text.lower() for token in parsed] #and (token.is_stop is False)]\n",
    "\n",
    "def remove_punc(parsed):\n",
    "    return [token.text for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)] #and (token.is_stop is False)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "   # tokenize each sentence -- each tokenized sentence will be an element in token_dataset\n",
    "    token_dataset = []\n",
    "    # tokenize all words -- each token will be an item in all_tokens (in the order given by the list of sentences)\n",
    "    all_tokens = []     # all the tokens -- \n",
    "\n",
    "    for sample in _tqdm(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "#         tokens = lower_case_remove_punc(sample)\n",
    "        tokens = lower_case(sample)       # make words lower case\n",
    "#         tokens = remove_punct(tokens)     # remove punctuation\n",
    "        token_dataset.append(tokens)    \n",
    "        all_tokens += tokens\n",
    "        \n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', '!', str, 32, str)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations, punctuations[0], \\\n",
    "type(punctuations), len(punctuations), type(punctuations[0]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<[^>]+>', re.UNICODE)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: for now only work with small subset of the data -- switch to all data later\n",
    "# train_data = train_data[:800]\n",
    "# test_data = test_data[:100]\n",
    "# valid_data = valid_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, str, str)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data), type(train_data[0]), type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b753a150042a7954dd544473e3864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c412200ce146a78621aba160b0dc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f752b4d1bd4ec4b69f375d72c076df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = tokenize_dataset(train_data)\n",
    "test_data_tokenized, all_tokens_test = tokenize_dataset(test_data)\n",
    "valid_data_tokenized, all_tokens_valid = tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokenized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16412250,\n",
       " 'this',\n",
       " 222919,\n",
       " ['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'look',\n",
       "  'cheap',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'so',\n",
       "  'glad',\n",
       "  'i',\n",
       "  'looked',\n",
       "  'on',\n",
       "  'amazon',\n",
       "  'and',\n",
       "  'found',\n",
       "  'such',\n",
       "  'an',\n",
       "  'affordable',\n",
       "  'tutu',\n",
       "  'that',\n",
       "  'isn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'made',\n",
       "  'poorly',\n",
       "  '.',\n",
       "  'a',\n",
       "  '+',\n",
       "  '+'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of All Tokens\n",
    "len(all_tokens_train), all_tokens_train[0], \\\n",
    "len(train_data_tokenized), train_data_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Vocabulary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 71128 words\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary using all the tokens found in train data (90% of most common ones)\n",
    "voc = list(set(all_tokens_train))\n",
    "print('Word vocabulary size: {} words'.format(len(voc)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS ANALYSIS (Train + Valid Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Tokens in the Corpus Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All Tokens  16412250\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All Tokens \", len(all_tokens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All UNIQUE Tokens  71128\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All UNIQUE Tokens \", len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Sentences in the Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences  222919\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Sentences \", len(train_data_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for padding the sentences with special markers sentence beginning and end, i.e. $<bos>$ and $<eos>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        padded = [\"<bos>\" for i in range((n - 1))] + l +[\"<eos>\" for i in range((n - 1))]\n",
    "        result_list.append(padded)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sentences(train_data_tokenized, n)\n",
    "valid_padded = pad_sentences(valid_data_tokenized, n)\n",
    "test_padded = pad_sentences(test_data_tokenized, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_padded[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for finding all N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append(list(zip(*[l[i:] for i in range(n)])))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram counts for already tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counts(data, n, frac_vocab=0.9):    \n",
    "    all_train_tokens = list(mit.flatten(data))\n",
    "    counted_tokens = Counter(all_train_tokens)\n",
    "    max_vocab_size = int(frac_vocab * len(counted_tokens))\n",
    "\n",
    "    vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "    \n",
    "    return vocab, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "train_padded = pad_sentences(train_data_tokenized, n)\n",
    "train_ngram = find_ngrams(train_padded, n)\n",
    "vocab_ngram, count_ngram = ngram_counts(train_ngram, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_padded, train_ngram, vocab_ngram, count_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams, Bigrams, Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_trigram = pad_sentences(train_data_tokenized, 3)\n",
    "train_trigram = find_ngrams(train_padded_trigram, 3)\n",
    "vocab_trigram, count_trigram = ngram_counts(train_trigram, 3)\n",
    "\n",
    "train_padded_bigram = pad_sentences(train_data_tokenized, 2)\n",
    "train_bigram = find_ngrams(train_padded_bigram, 2)\n",
    "vocab_bigram, count_bigram = ngram_counts(train_bigram, 2)\n",
    "\n",
    "train_padded_unigram = pad_sentences(train_data_tokenized, 1)\n",
    "train_unigram = find_ngrams(train_padded_unigram, 1)\n",
    "vocab_unigram, count_unigram = ngram_counts(train_unigram, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_bigram[:3], count_bigram[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_unigram[:3], count_unigram[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1 \n",
    "BOS_IDX = 2\n",
    "EOS_IDX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_dict(vocab):\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1 \n",
    "    BOS_IDX = 2\n",
    "    EOS_IDX = 3\n",
    "    \n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(4, 4+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>', '<bos>', '<eos>'] + id2token\n",
    "\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    token2id['<bos>'] = BOS_IDX \n",
    "    token2id['<eos>'] = EOS_IDX\n",
    "\n",
    "    return id2token, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token_ngram, token2id_ngram = ngram_dict(vocab_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<pad>',\n",
       "  '<unk>',\n",
       "  '<bos>',\n",
       "  '<eos>',\n",
       "  ('.', '<eos>', '<eos>'),\n",
       "  ('<bos>', '<bos>', 'i'),\n",
       "  ('.', '.', '.'),\n",
       "  ('it', \"'\", 's'),\n",
       "  ('!', '<eos>', '<eos>'),\n",
       "  ('i', \"'\", 'm')],)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token_ngram[:10], \\\n",
    "# token2id_ngram['<unk>'], token2id_ngram['<eos>'], token2id_ngram[('rosetta', 'stone')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 3875499 ; token ('rapida', 'la', 'compra')\n",
      "Token ('rapida', 'la', 'compra'); token id 3875499\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token_ngram) - 1)\n",
    "random_token = id2token_ngram[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_ngram[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_ngram[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function that combines all the above and goes from tokenized data to the ngram dataset\n",
    "# def create_id_dataset(data, n):\n",
    "#     padded_data = pad_sentences(data, n)\n",
    "#     ngram_data = find_ngrams(padded_data, n)\n",
    "    \n",
    "#     vocab, count = ngram_counts(ngram_data, n)    \n",
    "#     id2token, token2id = ngram_dict(vocab)\n",
    "    \n",
    "#     data_id = create_data_id(ngram_data, token2id)\n",
    "#     data_id_merged = create_data_id_merged(data_id, token2id, n)\n",
    "    \n",
    "#     return data_id, data_id_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data_id, all_data_id_merged = create_id_dataset(train_data_tokenized, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('.', '<eos>', '<eos>'),\n",
       "  ('<bos>', '<bos>', 'i'),\n",
       "  ('.', '.', '.'),\n",
       "  ('it', \"'\", 's'),\n",
       "  ('!', '<eos>', '<eos>'),\n",
       "  ('i', \"'\", 'm'),\n",
       "  ('don', \"'\", 't'),\n",
       "  ('&', '#', '34'),\n",
       "  ('#', '34', ';'),\n",
       "  ('<bos>', '<bos>', 'this')),\n",
       " (161218, 77237, 47023, 38312, 32534, 30372, 28225, 26413, 26413, 23103))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ngram[:10], count_ngram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_count(ngram, vocab, count):\n",
    "    if ngram in vocab:\n",
    "        ngram_idx = vocab.index(ngram)\n",
    "        return count[ngram_idx] \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_ngram_count(('i', 'like', 'this'), vocab_ngram, count_ngram)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = get_ngram_count(('i', 'like', 'pandas'), vocab_ngram, count_ngram)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the probability of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(w|w_{−n}, ..., w_{−2}, w_{−1}) \\approx \\frac{c(w_{−n}, ..., w_{−2}, w_{−1}, w)}{\\sum_{w \\in V} c(w_{−n}, ..., w_{−2}, w_{−1}, w)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob(ngram, vocab, count):\n",
    "    c = get_ngram_count(ngram, vocab, count)\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "#             print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Probabilities\n",
    "\n",
    "## $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{\\sum_{w_i} c(w_{i-1}, w_i)} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13043478260869565"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob(('rosetta', 'stone', 'is'), vocab_ngram, count_ngram)\n",
    "p\n",
    "\n",
    "# p = get_ngram_prob(('i', 'am', 'rosetta'), vocab_ngram, count_ngram)\n",
    "# p\n",
    "\n",
    "# p = get_ngram_prob(('it', \"'\", 's'), vocab_ngram, count_ngram)\n",
    "# p\n",
    "\n",
    "# p = get_ngram_prob(('i', \"like\", 'this'), vocab_ngram, count_ngram)\n",
    "# p, 1/(2+1+1+1+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_addditive_smoothing(ngram, vocab, count, delta=0.5):\n",
    "    c = get_ngram_count(ngram, vocab, count) + delta*1\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "#             print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    all_counts += delta*len(voc)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4059160949274547e-05"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_addditive_smoothing(('am', 'rosetta', 'stone'), vocab_ngram, count_ngram, delta=0.5)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-One Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_add_one_smoothing(ngram, vocab, count):\n",
    "    c = get_ngram_count(ngram, vocab, count) + 1\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "#             print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    all_counts += len(voc)\n",
    "    if all_counts > 0:\n",
    "        return c / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4059160949274547e-05"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_add_one_smoothing(('am', 'rosetta', 'stone'), vocab_ngram, count_ngram)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Interpolation Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: add formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-07e3e8bf60ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_padded_unigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_unigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_padded_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvocab_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_unigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-ea1b6fc15f5e>\u001b[0m in \u001b[0;36mngram_counts\u001b[0;34m(data, n, frac_vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mngram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mall_train_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcounted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac_vocab\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounted_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_padded_trigram = pad_sentences(train_data_tokenized, 3)\n",
    "train_trigram = find_ngrams(train_padded_trigram, 3)\n",
    "vocab_trigram, count_trigram = ngram_counts(train_trigram, 3)\n",
    "\n",
    "train_padded_bigram = pad_sentences(train_data_tokenized, 2)\n",
    "train_bigram = find_ngrams(train_padded_bigram, 2)\n",
    "vocab_bigram, count_bigram = ngram_counts(train_bigram, 2)\n",
    "\n",
    "train_padded_unigram = pad_sentences(train_data_tokenized, 1)\n",
    "train_unigram = find_ngrams(train_padded_unigram, 1)\n",
    "vocab_unigram, count_unigram = ngram_counts(train_unigram, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob_interpolation_smoothing(ngram, vocab, count, prev_vocab, prev_count, alpha=0.5):\n",
    "    c = get_ngram_count(ngram, vocab, count)\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        if t[:-1] == ngram[:-1]:\n",
    "#             print(t, get_ngram_count(t, vocab, count))\n",
    "            all_counts += get_ngram_count(t, vocab, count)\n",
    "    if all_counts > 0:\n",
    "        prob_ngram = c / all_counts\n",
    "    else:\n",
    "        prob_ngram = 0\n",
    "    \n",
    "    prev_ngram = tuple(list(ngram[1:]))\n",
    "    prev_c = get_ngram_count(prev_ngram, prev_vocab, prev_count)\n",
    "#     print(prev_c)\n",
    "    prev_all_counts = 0\n",
    "    for prev_t in prev_vocab:\n",
    "        if prev_t[:-1] == prev_ngram[:-1]:\n",
    "#             print(prev_t, get_ngram_count(prev_t, prev_vocab, prev_count))\n",
    "            prev_all_counts += get_ngram_count(prev_t, prev_vocab, prev_count)\n",
    "    if prev_all_counts > 0:\n",
    "        prob_prev_ngram = prev_c / prev_all_counts\n",
    "    else:\n",
    "        0\n",
    "    return alpha*(prob_ngram) + (1-alpha)*prob_prev_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17468354430379743"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = get_ngram_prob_interpolation_smoothing(('am', 'rosetta', 'stone'), vocab_trigram, count_trigram, vocab_bigram, count_bigram, alpha=0.8)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing: Linear Interpolation with Absolute Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$p_{bi}(w|v) = max ({ \\frac{N(v, w) - b_{bi}}{N(v)}, 0)  + b_{bi} \\frac{V - N_0(v, \\cdot)}{N(v)} p_{uni}(w) \\large}$$\n",
    "\n",
    "### $$p_{uni}(w) = max ({ \\frac{N(w) - b_{uni}}{N}, 0)  + b_{uni} \\frac{V - N_0(\\cdot)}{N} \\frac{1}{V}}$$\n",
    "\n",
    "### $$b_{bi} = \\frac{N_1(\\cdot, \\cdot)}{N_1(\\cdot, \\cdot) + 2*N_2(\\cdot, \\cdot)}$$\n",
    "\n",
    "### $$b_{uni} = \\frac{N_1(\\cdot)}{N_1(\\cdot) + 2*N_2(\\cdot)}$$\n",
    "\n",
    "\n",
    "### $$N_r(\\cdot) = \\sum_{w: N(w) = r} 1$$\n",
    "\n",
    "### $$N_r(\\cdot, \\cdot) = \\sum_{v, w: N(v, w) = r} 1$$\n",
    "\n",
    "### $$N_r(v, \\cdot) = \\sum_{w: N(v, w) = r} 1$$\n",
    "\n",
    "### V is the number of words in the vocabulary\n",
    "\n",
    "### $N_r(\\cdot, \\cdot)$ and $N_r(\\cdot)$  are the count-counts for bigrams and unigrams respectively $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_count(r):\n",
    "    return np.sum([1 for i in range(len(vocab_unigram)) if count_unigram[i] == r])\n",
    "\n",
    "def get_bigram_count(r):\n",
    "    return np.sum([1 for i in range(len(vocab_bigram)) if count_bigram[i] == r])\n",
    "\n",
    "def get_biunigram_count(r, token):\n",
    "    cc = 0\n",
    "    for other_token in vocab_unigram:\n",
    "        bigram = tuple([token] + [other_token])\n",
    "        if bigram in vocab_bigram:\n",
    "            bigram_idx = vocab_bigram.index(bigram) \n",
    "            if count_bigram[bigram_idx] == r:\n",
    "                cc += 1\n",
    "                \n",
    "#     for bigram in vocab_bigram:\n",
    "#         print(token, bigram[0])\n",
    "#         if token == bigram[0]:\n",
    "#             bigram_idx = vocab_bigram.index(bigram) \n",
    "#             if count_bigram[bigram_idx] == r:\n",
    "#                 cc += 1\n",
    "    return cc\n",
    "\n",
    "def get_b_bi():\n",
    "    bbi = get_bigram_count(1) / (get_bigram_count(1) + 2 * get_bigram_count(2))\n",
    "    return bbi\n",
    "    \n",
    "def get_b_uni():\n",
    "    buni = get_unigram_count(1) / (get_unigram_count(1) + 2 * get_unigram_count(2))\n",
    "    return buni\n",
    "\n",
    "def get_p_uni(w):\n",
    "    if w in vocab_unigram:\n",
    "        w_idx = vocab_unigram.index(w)\n",
    "        N_w = count_unigram[w_idx]\n",
    "    else:\n",
    "        N_w = 0\n",
    "        \n",
    "    b_uni = get_b_uni()\n",
    "    \n",
    "    W = len(voc)\n",
    "    N_0 = get_unigram_count(0)\n",
    "    \n",
    "    \n",
    "    N = len(all_tokens_train) # TODO: double check the meaning of N \n",
    "    \n",
    "    p_uni = max((N_w - b_uni / N), 0) + b_uni * (W - N_0) / N * 1 / W\n",
    "    \n",
    "    return p_uni\n",
    "\n",
    "def get_p_bi(w, v):   # w given v\n",
    "    if tuple([v] + [w]) in vocab_bigram:\n",
    "        vw_idx = vocab_bigram.index(tuple([v] + [w]))\n",
    "        N_vw = count_bigram[vw_idx]\n",
    "    else:\n",
    "        N_vw = 0\n",
    "        \n",
    "    if tuple([v]) in vocab_unigram:\n",
    "        v_idx = vocab_unigram.index(tuple([v]))\n",
    "        N_v = count_unigram[v_idx]\n",
    "    else:\n",
    "        N_v = 0  \n",
    "        \n",
    "    b_bi = get_b_bi()\n",
    "    b_uni = get_b_uni()\n",
    "    \n",
    "    p_uni = get_p_uni(tuple([w]))\n",
    "    \n",
    "    W = len(voc)\n",
    "    N_0 = get_biunigram_count(0, v)\n",
    "    \n",
    "    \n",
    "    p_bi =  max((N_vw - b_bi) / N_v,  0) + \\\n",
    "         b_bi * (W - N_0) / N_v * p_uni\n",
    "    \n",
    "    return p_bi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-9a175d9fa472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rosetta'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_p_bi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-1e0484bf6231>\u001b[0m in \u001b[0;36mget_p_bi\u001b[0;34m(w, v)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mN_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_biunigram_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-1e0484bf6231>\u001b[0m in \u001b[0;36mget_biunigram_count\u001b[0;34m(r, token)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mother_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_unigram\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mother_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_bigram\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mbigram_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_bigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_bigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = 'stone'\n",
    "y = 'rosetta'\n",
    "\n",
    "z = get_p_bi(y, x)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that the probabilities sum up to one\n",
    "### $$\\sum_w p_{bi}(w|v) = \\sum_w p_{uni}(w) = 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add this check or leave as homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram LM\n",
    "###  $$p(s) = \\prod_{i = 1} ^ {N + 1} p(w_i | w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_sentence(sentence, vocab, count, n):\n",
    "    padded_sentence = pad_sentences(sentence, n)  # needs a list\n",
    "#     print(padded_sentence)\n",
    "    ngram_sentence = find_ngrams(padded_sentence, n)[0] # only one element in list\n",
    "#     print(ngram_sentence)\n",
    "    prob = 1\n",
    "    for ngram in ngram_sentence:\n",
    "        prob_ngram = get_ngram_prob(ngram, vocab, count)\n",
    "#         print(ngram, prob_ngram)\n",
    "        prob *= prob_ngram\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'a', 'great', 'tutu', 'and', 'at', 'a', 'really', 'great', 'price', '.', 'it', 'doesn', \"'\", 't', 'look', 'cheap', 'at', 'all', '.', 'i', \"'\", 'm', 'so', 'glad', 'i', 'looked', 'on', 'amazon', 'and', 'found', 'such', 'an', 'affordable', 'tutu', 'that', 'isn', \"'\", 't', 'made', 'poorly', '.', 'a', '+', '+']]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "sentence = [train_data_tokenized[0]]\n",
    "print(sentence)\n",
    "ps = get_prob_sentence(sentence, vocab_ngram, count_ngram, n)\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "sentence = [['this', 'is', 'a', 'great', 'tutu']]\n",
    "print(sentence)\n",
    "ps = get_prob_sentence(sentence, vocab_ngram, count_ngram, n)\n",
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "### Bigram LM: $$ p(i \\; love \\; this \\; light) = p(i|\\cdot) \\; p(love|i)\\;  p(this|love)\\;  p(light|this) \\\\\n",
    "\\approx \\frac{c(i, \\cdot)}{\\sum_w c(\\cdot, \\; w)} \\; \\frac{c(love, i)}{\\sum_wc(i, \\; w)}\\;  \\frac{c(this, love)}{\\sum_wc(love, \\;w)}\\;  \\frac{c(light, this)}{\\sum_wc(this, \\;w)}$$ \n",
    "\n",
    "### Trigram LM: $$ p(i \\; love \\; this  \\;light) = p(i|\\cdot, \\cdot) \\; p(love|\\cdot, i) \\; p(this|i, love)\\;  p(light|love, this)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=False):\n",
    "    pd = [0 for v in voc]\n",
    "    for idx, token in enumerate(voc):\n",
    "#         print(\"token: \", token)\n",
    "#         print(\"prev ngram: \", prev_tokens)\n",
    "#         print(\"both: \", tuple(list(prev_tokens) + [token]))\n",
    "#         print(\"\")\n",
    "        token_ngram = tuple(list(prev_tokens) + [token])\n",
    "        pd[idx] = get_ngram_prob(token_ngram, vocab_ngram, count_ngram)\n",
    "#         if pd[idx] > 0 and print_nonzero_probs:\n",
    "#             print(token_ngram, \" \", pd[idx])\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob distr for the word following prev_tokens (i.e. tutu) \n",
    "# over all the words in the vocabulary \n",
    "\n",
    "# prev_tokens = train_data_tokenized[0][4] #[0]\n",
    "prev_tokens = vocab_ngram[3][1:] #[0]   # need frmo 1 on so that this is a correct prev token\n",
    "print(prev_tokens)\n",
    "pd = get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=True)\n",
    "sum(pd)#, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=False):\n",
    "    pd = get_prob_distr_ngram(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=print_nonzero_probs)\n",
    "    idx_next_token = np.random.choice(len(voc), 1, p=pd)[0]\n",
    "    return voc[idx_next_token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prev_tokens)\n",
    "next_token = sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc, print_nonzero_probs=True)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(num_tokens, vocab_ngram, count_ngram, voc, n):\n",
    "    sentence = []\n",
    "    prev_tokens = tuple(['<bos>'] * (n - 1))\n",
    "#     print(prev_tokens)\n",
    "    for i in range(num_tokens):\n",
    "        next_token = sample_from_pd(prev_tokens, vocab_ngram, count_ngram, voc)\n",
    "#         print(i, next_token)\n",
    "#         print(i, prev_tokens[1:])\n",
    "        prev_tokens = tuple(list(prev_tokens[1:]) + [next_token])\n",
    "#         print(i, prev_tokens)\n",
    "        sentence.append(next_token)\n",
    "        print(' '.join(sentence))\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 5\n",
    "generated_sentence = generate_sentence(num_tokens, vocab_ngram, count_ngram, voc, n)\n",
    "generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 5\n",
    "generated_sentence = generate_sentence(num_tokens, vocab_ngram, count_ngram, voc, n)\n",
    "generated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "# show rank for each word in a sentence\n",
    "# explain perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "### $LL = \\sum_{k=1}^{K} \\sum_{n=1}^{N_k + 1} log p_{bi}(w_{k,n} | w_{k,n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $PP = exp(-\\frac{LL}{\\sum_k(N_k + 1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(test_sentences, vocab_ngram, count_ngram):\n",
    "    ll = 0\n",
    "    num_tokens = 0\n",
    "    for s in (test_sentences):\n",
    "        ll += get_prob_sentence([s], vocab_ngram, count_ngram)\n",
    "        num_tokens += len(s) + 1\n",
    "\n",
    "    ppl = np.exp(-ll/num_tokens)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_test = get_perplexity(test_data_tokenized, vocab_ngram, count_ngram)\n",
    "ppl_valid = get_perplexity(valid_data_tokenized, vocab_ngram, count_ngram)\n",
    "ppl_train = get_perplexity(train_data_tokenized, vocab_ngram, count_ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_test, ppl_valid, ppl_train\n",
    "# TODO check whether this makes sense -- maybe it seems too good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some examples and see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a PyTorch Dataset out of our set of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, use_cuda=True):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n",
    "        for (i, t) in tqdm_notebook(self.data):\n",
    "            print(i, t)\n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), \\\n",
    "                                        torch.LongTensor([t]).to(device)))\n",
    "                \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        \n",
    "        return inp, tgt, len(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-1]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_id(data, token2id):\n",
    "    data_id = []\n",
    "    for d in data:\n",
    "        data_id.append(_text2id(d, token2id))\n",
    "    return data_id\n",
    "\n",
    "def create_data_id_merged(data, token2id, n):\n",
    "    data_id_merged = []\n",
    "    for d in data:\n",
    "        for i in range(len(d) - n):\n",
    "            data_id_merged.append((d[i:i+n], d[i+n]))\n",
    "    return data_id_merged\n",
    "\n",
    "n = 1\n",
    "train_padded_uni = pad_sentences(train_data_tokenized, n)\n",
    "train_unigram = find_ngrams(train_padded_uni, n)\n",
    "train_vocab_unigram, train_count_unigram = ngram_counts(train_unigram, n)\n",
    "train_id2token_unigram, train_token2id_unigram = ngram_dict(train_vocab_unigram)\n",
    "\n",
    "n = 1\n",
    "valid_padded_uni = pad_sentences(valid_data_tokenized, n)\n",
    "valid_unigram = find_ngrams(valid_padded_uni, n)\n",
    "valid_vocab_unigram, count_unigram = ngram_counts(valid_unigram, n)\n",
    "valid_id2token_unigram, valid_token2id_unigram = ngram_dict(valid_vocab_unigram)\n",
    "\n",
    "N = 10\n",
    "train_data_id = create_data_id(train_unigram, train_token2id_unigram)\n",
    "train_data_id_merged = create_data_id_merged(train_data_id, train_token2id_unigram, N)\n",
    "\n",
    "valid_data_id = create_data_id(valid_unigram, valid_token2id_unigram)\n",
    "valid_data_id_merged = create_data_id_merged(valid_data_id, valid_token2id_unigram, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AmazonDataset(train_data_id_merged, max_inp_length=None, use_cuda=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=batchify, shuffle=True)\n",
    "\n",
    "valid_dataset = AmazonDataset(valid_data_id_merged, max_inp_length=None, use_cuda=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_id_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0], train_dataset[0][0].shape, \\\n",
    "valid_dataset[0], valid_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ATBQ4WaJR93"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfNGrams(nn.Module):\n",
    "    def init_layers(self):\n",
    "        for l in self.layers:\n",
    "            if getattr(l, 'weight', None) is not None:\n",
    "                torch.nn.init.xavier_uniform_(l.weight)\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim=300, hidden_size=256, out_size=128, reduce='sum', nlayers=2, activation='ReLU', dropout=0.1, batch_norm=False):\n",
    "        super(BagOfNGrams, self).__init__()\n",
    "       \n",
    "        self.emb_dim = emb_dim\n",
    "        self.reduce = reduce\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.activation = getattr(nn, activation)\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_dim, mode=reduce)\n",
    "        if batch_norm is True:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.emb_dim)\n",
    "        self.layers = nn.ModuleList([nn.Linear(self.emb_dim, self.hidden_size)])\n",
    "        self.layers.append(self.activation())\n",
    "        self.layers.append(nn.Dropout(p=dropout))\n",
    "        \n",
    "        for i in range(self.nlayers-2):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(self.activation())\n",
    "            self.layers.append(nn.Dropout(p=dropout)) \n",
    "        self.layers.append(nn.Linear(self.hidden_size, self.out_size))\n",
    "        self.init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        postemb = self.embedding(x)\n",
    "        if hasattr(self, 'batch_norm'):\n",
    "            x = self.batch_norm(postemb)\n",
    "        else:\n",
    "            x = postemb\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderMLP(nn.Module):\n",
    "    \"\"\"Generates a token in response to context.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=128, output_size=1024, hidden_size=256):\n",
    "        \"\"\"Initialize decoder.\n",
    "        :param input_size: size of embedding\n",
    "        :param output_size: size of vocabulary\n",
    "        :param hidden_size: size of the linear layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "            \n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Return encoded state.\n",
    "        :param input: batch_size x 1 tensor of token indices.\n",
    "        :param hidden: past (e.g. encoder) hidden state\n",
    "        \"\"\"\n",
    "        output = F.relu(self.linear(input))\n",
    "        scores = self.softmax(self.out(output))\n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, bag_of_ngrams, decoder, lr = 1e-3, use_cuda = True, \n",
    "                        longest_label = 20, \n",
    "                        clip = 0.3):\n",
    "        super(seq2seq, self).__init__()\n",
    "\n",
    "        device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n",
    "        self.device = device;\n",
    "        self.bag_of_ngrams = bag_of_ngrams.to(device);\n",
    "        self.decoder = decoder.to(device)\n",
    "\n",
    "        self.longest_label = longest_label\n",
    "\n",
    "        # set up the criterion\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "        self.optims = {\n",
    "             'nmt': optim.SGD(self.parameters(), lr=lr, nesterov=True, momentum = 0.99)\n",
    "        }\n",
    "\n",
    "        self.longest_label = longest_label\n",
    "        self.clip = clip;\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out optimizer.\"\"\"\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def update_params(self):\n",
    "        \"\"\"Do one optimization step.\"\"\"\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.bag_of_ngrams.parameters(), self.clip)\n",
    "            torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), self.clip)\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "    \n",
    "    def v2t(self, vector):\n",
    "        return [train_id2token_unigram[i] for i in vector]\n",
    "        \n",
    "    def train_step(self, xs, ys):\n",
    "        \"\"\"Train model to produce ys given xs.\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        Return estimated responses, with teacher forcing on the input sequence\n",
    "        (list of strings of length batchsize).\n",
    "        \"\"\"\n",
    "        if xs is None:\n",
    "            return\n",
    "        xs = xs.to(self.device)\n",
    "        ys = ys.to(self.device)\n",
    "\n",
    "        self.zero_grad()\n",
    "        self.bag_of_ngrams.train()\n",
    "        self.decoder.train()\n",
    "    \n",
    "        bow_output = self.bag_of_ngrams(xs)\n",
    "        decoder_output = self.decoder(bow_output)\n",
    "            \n",
    "        loss = self.criterion(decoder_output, ys.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "\n",
    "        _max_score, predictions = decoder_output.max(1)\n",
    "        \n",
    "        return self.v2t(predictions), loss.item() \n",
    "\n",
    "    def eval_step(self):\n",
    "        \"\"\"Generate a response to the input tokens.\n",
    "        :param batch: parlai.core.torch_agent.Batch, contains tensorized\n",
    "                      version of observations.\n",
    "        Return predicted responses (list of strings of length batchsize).\n",
    "        \"\"\"\n",
    "        # just predict\n",
    "        self.bag_of_ngrams.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        predictions = []\n",
    "        encoder_input = torch.LongTensor([BOS_IDX] * N).unsqueeze(0)\n",
    "\n",
    "        for _ in range(self.longest_label):\n",
    "            decoder_input = self.bag_of_ngrams(encoder_input)\n",
    "            decoder_output = self.decoder(decoder_input)\n",
    "            _max_score, next_token = decoder_output.max(1)\n",
    "            \n",
    "            predictions.append(next_token)\n",
    "            \n",
    "            prev_tokens = torch.cat([encoder_input.squeeze(0)[1:N]])\n",
    "\n",
    "            encoder_input = torch.cat((prev_tokens, next_token), 0).unsqueeze(0)\n",
    "            \n",
    "            # stop if you've found the \n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "                \n",
    "        predictions = torch.cat(predictions, 0)\n",
    "        return self.v2t(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "bag_of_ngrams = BagOfNGrams(len(train_id2token_unigram), emb_dim=300, hidden_size=256, out_size=128, activation='Tanh', nlayers=1, reduce='mean', dropout=0.0, batch_norm=False)\n",
    "decoder = DecoderMLP(input_size=128, output_size=len(train_id2token_unigram), hidden_size=256)\n",
    "model = seq2seq(bag_of_ngrams, decoder, use_cuda=False, lr=1e-1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss_epoch = 0\n",
    "    for i, (data, labels) in _tqdm(enumerate(train_loader)):\n",
    "        prediction, loss = model.train_step(data, labels)\n",
    "        train_loss_epoch += loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {}: Loss {}\".format(epoch, train_loss_epoch))\n",
    "#         print(\"prediction \", prediction)\n",
    "\n",
    "        generated = model.eval_step()\n",
    "        generated_str = ' '.join([g[0] for g in generated])\n",
    "        print(\"Generated Sentence: \", generated_str)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ngram-lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
