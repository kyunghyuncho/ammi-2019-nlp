{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/ngram_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "## Goal: compute a probabilty distribution over all possible sentences:\n",
    "\n",
    "\n",
    "## $$p(W) = p(w_1, w_2, ..., w_T)$$\n",
    "\n",
    "## This unsupervised learning problem can be framed as a sequence of supervised learning problems:\n",
    "\n",
    "## $$p(W) = p(w_1) * p(w_2|w_1) * ... * p(w_T|w_1, ..., w_{T-1})$$\n",
    "\n",
    "## If we have N sentences, each of them with T words / tokens, then we want to max:\n",
    "\n",
    "## $$log p(W) = \\sum_{n = 1}^N \\sum_{i=1}^{T} log p(w_i | w_{<i})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram language model\n",
    "\n",
    "## Goal: estimate the n-gram probabilities using counts of sequences of n consecutive words\n",
    "\n",
    "## Given a sequence of words $w$, we want to compute\n",
    "\n",
    "##  $$P(w_i|w_{i−1}, w_{i−2}, …, w_{i−n+1})$$\n",
    "\n",
    "## Where $w_i$ is the i-th word of the sequence.\n",
    "\n",
    "## $$P(w_i|w_{i−n+1}, ..., w_{i−2}, w_{i−1}) = \\frac{p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w_i)}{\\sum_{w \\in V} p(w_{i−n+1}, ..., w_{i−2}, w_{i−1}, w)}$$\n",
    "\n",
    "## Key Idea: We can estimate the probabilities using counts of n-grams in our dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see this in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: should we install as needed and import as needed or all at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run if you dont have it installed\n",
    "# !pip install more_itertools\n",
    "# !pip install spacy# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\\\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCVSciOCAMZb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYs6AMs6AIre"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy\n",
    "import itertools\n",
    "from operator import itemgetter \n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import re\n",
    "import more_itertools as mit  # not built-in package\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext import vocab\n",
    "from collections import Counter\n",
    "import re\n",
    "from torchtext.data import TabularDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H20pktPiA63a",
    "outputId": "fb38d897-e889-4451-df77-9ca98eb266a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6fdd852830>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From .txt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "train_data = open('../data/amazon_reviews_sports_train.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237069,\n",
       " list,\n",
       " \"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\\n\",\n",
       " str,\n",
       " 227)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Train Data        \n",
    "len(train_data), type(train_data), train_data[0], type(train_data[0]), len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "valid_data = open('../data/amazon_reviews_sports_valid.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29635,\n",
       " list,\n",
       " \"['great', 'sling', 'bag', '.', 'i', 'used', 'it', 'for', 'a', 'trip', 'to', 'disney', 'and', 'this', 'bag', 'worked', 'very', 'well', '.', 'it', 'has', 'enough', 'pockets', 'to', 'carry', 'all', 'the', 'essentials', 'needed', 'for', 'a', 'vacation', 'to', 'disney', 'in', 'june', '.', 'i', 'was', 'able', 'to', 'carry', 'so', 'mush', 'stuff', 'in', 'this', 'bag', 'my', 'wife', 'did', 'not', 'have', 'to', 'carry', 'a', 'back', 'pack', '.', 'i', 'gonna', 'to', 'carry', 'this', 'bag', 'for', 'all', 'my', 'trips', 'from', 'now', 'on', 'and', 'it', 'worked', 'well', 'through', 'all', 'the', 'airport', 'security', 'checks', '.', 'i', 'highly', 'recommend', 'this', 'bag', '.']\\n\",\n",
       " str,\n",
       " 677)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Valid Data\n",
    "len(valid_data), type(valid_data), valid_data[0], type(valid_data[0]), len(valid_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "test_data = open('../data/amazon_reviews_sports_test.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29633,\n",
       " list,\n",
       " '[\\'this\\', \\'is\\', \\'the\\', \\'most\\', \\'affordable\\', \\'/\\', \\'highest\\', \\'quality\\', \\'made\\', \\'in\\', \\'the\\', \\'usa\\', \\'knife\\', \\'i\\', \\'have\\', \\'found\\', \\'.\\', \\'the\\', \\'knife\\', \\'is\\', \\'very\\', \\'lightweight\\', \\',\\', \\'lighter\\', \\'than\\', \\'my\\', \\'kershaw\\', \\'blur\\', \\'and\\', \\'its\\', \\'a\\', \\'bit\\', \\'thinner\\', \\'too\\', \\'.\\', \\'typically\\', \\'really\\', \\'lightweight\\', \\'knives\\', \\'have\\', \\'frn\\', \\'handles\\', \\'like\\', \\'the\\', \\'endura\\', \\'4\\', \\'or\\', \\'the\\', \\'griptillian\\', \\'and\\', \\'that\\', \"\\'\", \\'s\\', \\'fine\\', \\'but\\', \\'that\\', \\'stuff\\', \\'does\\', \\'flex\\', \\'if\\', \\'u\\', \\'squeeze\\', \\'hard\\', \\'enough\\', \\'.\\', \\'the\\', \\'aluminum\\', \\'handles\\', \\'in\\', \\'the\\', \\'knockout\\', \\'feel\\', \\'really\\', \\'strong\\', \\'and\\', \\'have\\', \\'no\\', \\'give\\', \\'to\\', \\'them\\', \\'and\\', \\'i\\', \\'prefer\\', \\'the\\', \\'feel\\', \\'of\\', \\'metal\\', \\'over\\', \\'plastic\\', \\'any\\', \\'day\\', \\'.\\', \\'there\\', \\'was\\', \\'a\\', \\'little\\', \\'rattle\\', \\'if\\', \\'i\\', \\'shook\\', \\'the\\', \\'blade\\', \\'but\\', \\'that\\', \\'was\\', \\'fixed\\', \\'with\\', \\'a\\', \\'little\\', \\'petroleum\\', \\'jelly\\', \\'on\\', \\'the\\', \\'torsion\\', \\'bar\\', \\'.\\', \\'the\\', \\'knockout\\', \\'could\\', \\'use\\', \\'some\\', \\'jimping\\', \\'on\\', \\'the\\', \\'spine\\', \\'and\\', \\'in\\', \\'places\\', \\'on\\', \\'the\\', \\'handle\\', \\'to\\', \\'make\\', \\'it\\', \\'less\\', \\'slick\\', \\'and\\', \\'that\\', \"\\'\", \\'s\\', \\'the\\', \\'only\\', \\'knock\\', \\'i\\', \\'can\\', \\'give\\', \\'the\\', \\'knockoutupdate\\', \\':\\', \\'the\\', \\'more\\', \\'i\\', \\'used\\', \\'this\\', \\'knife\\', \\',\\', \\'the\\', \\'more\\', \\'i\\', \\'loved\\', \\'it\\', \\'.\\', \\'after\\', \\'a\\', \\'while\\', \\'you\\', \\'get\\', \\'used\\', \\'to\\', \\'no\\', \\'jimping\\', \\'and\\', \\'its\\', \\'fine\\', \\',\\', \\'the\\', \\'knife\\', \\'has\\', \\'remained\\', \\'rock\\', \\'solid\\', \\'with\\', \\'no\\', \\'bladeplay\\', \\',\\', \\'and\\', \\'it\\', \\'keeps\\', \\'an\\', \\'excellent\\', \\'edge\\', \\'.\\', \\'if\\', \\'i\\', \\'ever\\', \\'lost\\', \\'this\\', \\'i\\', \"\\'\", \\'d\\', \\'be\\', \\'furious\\', \\'but\\', \\'i\\', \\'would\\', \\'absolutely\\', \\'buy\\', \\'another\\', \\'one\\', \\'.\\']\\n',\n",
       " str,\n",
       " 1646)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Test Data\n",
    "len(test_data), type(test_data), test_data[0], type(test_data[0]), len(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')               \n",
    "punctuations = string.punctuation   \n",
    "TAG_RE = re.compile(r'<[^>]+>') # get rid off HTML tags from the data\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)] #and (token.is_stop is False)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "   # tokenize each sentence -- each tokenized sentence will be an element in token_dataset\n",
    "    token_dataset = []\n",
    "    # tokenize all words -- each token will be an item in all_tokens (in the order given by the list of sentences)\n",
    "    all_tokens = []     # all the tokens -- \n",
    "\n",
    "    for sample in tqdm(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<[^>]+>', re.UNICODE)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\\n\",\n",
       " '[\\'i\\', \\'had\\', \\'a\\', \\'factory\\', \\'glock\\', \\'tool\\', \\'that\\', \\'i\\', \\'was\\', \\'using\\', \\'for\\', \\'my\\', \\'glock\\', \\'26\\', \\',\\', \\'27\\', \\',\\', \\'and\\', \\'17\\', \\'.\\', \\'i\\', \"\\'\", \\'ve\\', \\'since\\', \\'lost\\', \\'it\\', \\'and\\', \\'had\\', \\'needed\\', \\'another\\', \\'.\\', \\'since\\', \\'i\\', \"\\'\", \\'ve\\', \\'used\\', \\'ghost\\', \\'products\\', \\'prior\\', \\',\\', \\'and\\', \\'know\\', \\'that\\', \\'they\\', \\'are\\', \\'reliable\\', \\',\\', \\'i\\', \\'had\\', \\'decided\\', \\'to\\', \\'order\\', \\'this\\', \\'one\\', \\'.\\', \\'sure\\', \\'enough\\', \\',\\', \\'this\\', \\'is\\', \\'just\\', \\'as\\', \\'good\\', \\'as\\', \\'a\\', \\'factory\\', \\'tool\\', \\'.\\']\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for now only work with small subset of the data -- switch to all data later\n",
    "train_data = train_data[:8000]\n",
    "test_data = test_data[:1000]\n",
    "valid_data = valid_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\\n\",\n",
       " 800)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [00:00, 881.28it/s]\n",
      "100it [00:00, 1409.27it/s]\n",
      "100it [00:00, 1125.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = tokenize_dataset(train_data)\n",
    "test_data_tokenized, all_tokens_test = tokenize_dataset(test_data)\n",
    "valid_data_tokenized, all_tokens_valid = tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokenized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,\n",
       " [['this',\n",
       "   'came',\n",
       "   'in',\n",
       "   'on',\n",
       "   'time',\n",
       "   'and',\n",
       "   'i',\n",
       "   'am',\n",
       "   'veru',\n",
       "   'happy',\n",
       "   'with',\n",
       "   'it',\n",
       "   'i',\n",
       "   'haved',\n",
       "   'used',\n",
       "   'it',\n",
       "   'already',\n",
       "   'and',\n",
       "   'it',\n",
       "   'makes',\n",
       "   'taking',\n",
       "   'out',\n",
       "   'the',\n",
       "   'pins',\n",
       "   'in',\n",
       "   'my',\n",
       "   'glock',\n",
       "   '32',\n",
       "   'very',\n",
       "   'easy',\n",
       "   '\\n'],\n",
       "  ['i',\n",
       "   'had',\n",
       "   'a',\n",
       "   'factory',\n",
       "   'glock',\n",
       "   'tool',\n",
       "   'that',\n",
       "   'i',\n",
       "   'was',\n",
       "   'using',\n",
       "   'for',\n",
       "   'my',\n",
       "   'glock',\n",
       "   '26',\n",
       "   '27',\n",
       "   'and',\n",
       "   '17',\n",
       "   'i',\n",
       "   've',\n",
       "   'since',\n",
       "   'lost',\n",
       "   'it',\n",
       "   'and',\n",
       "   'had',\n",
       "   'needed',\n",
       "   'another',\n",
       "   'since',\n",
       "   'i',\n",
       "   've',\n",
       "   'used',\n",
       "   'ghost',\n",
       "   'products',\n",
       "   'prior',\n",
       "   'and',\n",
       "   'know',\n",
       "   'that',\n",
       "   'they',\n",
       "   'are',\n",
       "   'reliable',\n",
       "   'i',\n",
       "   'had',\n",
       "   'decided',\n",
       "   'to',\n",
       "   'order',\n",
       "   'this',\n",
       "   'one',\n",
       "   'sure',\n",
       "   'enough',\n",
       "   'this',\n",
       "   'is',\n",
       "   'just',\n",
       "   'as',\n",
       "   'good',\n",
       "   'as',\n",
       "   'a',\n",
       "   'factory',\n",
       "   'tool',\n",
       "   '\\n']],\n",
       " [\"['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', ',', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy']\\n\",\n",
       "  '[\\'i\\', \\'had\\', \\'a\\', \\'factory\\', \\'glock\\', \\'tool\\', \\'that\\', \\'i\\', \\'was\\', \\'using\\', \\'for\\', \\'my\\', \\'glock\\', \\'26\\', \\',\\', \\'27\\', \\',\\', \\'and\\', \\'17\\', \\'.\\', \\'i\\', \"\\'\", \\'ve\\', \\'since\\', \\'lost\\', \\'it\\', \\'and\\', \\'had\\', \\'needed\\', \\'another\\', \\'.\\', \\'since\\', \\'i\\', \"\\'\", \\'ve\\', \\'used\\', \\'ghost\\', \\'products\\', \\'prior\\', \\',\\', \\'and\\', \\'know\\', \\'that\\', \\'they\\', \\'are\\', \\'reliable\\', \\',\\', \\'i\\', \\'had\\', \\'decided\\', \\'to\\', \\'order\\', \\'this\\', \\'one\\', \\'.\\', \\'sure\\', \\'enough\\', \\',\\', \\'this\\', \\'is\\', \\'just\\', \\'as\\', \\'good\\', \\'as\\', \\'a\\', \\'factory\\', \\'tool\\', \\'.\\']\\n'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Tokenized Sentences == Number of All Sentences in the Dataset\n",
    "# Compare the Tokenized Sentences with the Original Ones\n",
    "len(train_data_tokenized), train_data_tokenized[:2], train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78453, ['this', 'came'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of All Tokens\n",
    "len(all_tokens_train), all_tokens_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Vocabulary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 6281 words\n"
     ]
    }
   ],
   "source": [
    "# TODO: do we use both train and valid and not test for this??\n",
    "voc = list(set(all_tokens_train + all_tokens_valid))\n",
    "print('Word vocabulary size: {} words'.format(len(voc)))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS ANALYSIS (Train + Valid Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Tokens in the Corpus Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All Tokens  85962\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All Tokens \", len(all_tokens_train) + len(all_tokens_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of All UNIQUE Tokens  6281\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of All UNIQUE Tokens \", len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Sentences in the Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences  800 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Sentences \", len(train_data_tokenized), len(valid_data_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count how often each sentence length occurs. Visualize this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of words and their corresponding frequencies. Which are the 10 most frequent words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for padding the sentences with special markers sentence beginning and end, i.e. $<bos>$ and $<eos>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        padded = [\"<bos>\" for i in range((n - 1))] + l +[\"<eos>\" for i in range((n - 1))]\n",
    "        result_list.append(padded)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM = 2  # change this to make different N grams for each token\n",
    "\n",
    "train_data_padded = pad_sentences(train_data_tokenized, NGRAM)\n",
    "test_data_padded = pad_sentences(test_data_tokenized, NGRAM)\n",
    "valid_data_padded = pad_sentences(valid_data_tokenized, NGRAM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our padding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " 'this',\n",
       " 'came',\n",
       " 'in',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'veru',\n",
       " 'happy',\n",
       " 'with',\n",
       " 'it',\n",
       " 'i',\n",
       " 'haved',\n",
       " 'used',\n",
       " 'it',\n",
       " 'already',\n",
       " 'and',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'taking',\n",
       " 'out',\n",
       " 'the',\n",
       " 'pins',\n",
       " 'in',\n",
       " 'my',\n",
       " 'glock',\n",
       " '32',\n",
       " 'very',\n",
       " 'easy',\n",
       " '\\n',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for finding all N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    result_list = []\n",
    "    for l in input_list:\n",
    "        result_list.append(list(zip(*[l[i:] for i in range(n)])))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the dataset to its corresponding n-gram version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM = 2  # change this to make different N grams for each token\n",
    "\n",
    "# now make train and valid dicts\n",
    "train_data_ngram = find_ngrams(train_data_padded, NGRAM)\n",
    "valid_data_ngram = find_ngrams(valid_data_padded, NGRAM)\n",
    "test_data_ngram = find_ngrams(test_data_padded, NGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our n-grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<bos>', 'this'),\n",
       " ('this', 'came'),\n",
       " ('came', 'in'),\n",
       " ('in', 'on'),\n",
       " ('on', 'time'),\n",
       " ('time', 'and'),\n",
       " ('and', 'i'),\n",
       " ('i', 'am'),\n",
       " ('am', 'veru'),\n",
       " ('veru', 'happy'),\n",
       " ('happy', 'with'),\n",
       " ('with', 'it'),\n",
       " ('it', 'i'),\n",
       " ('i', 'haved'),\n",
       " ('haved', 'used'),\n",
       " ('used', 'it'),\n",
       " ('it', 'already'),\n",
       " ('already', 'and'),\n",
       " ('and', 'it'),\n",
       " ('it', 'makes'),\n",
       " ('makes', 'taking'),\n",
       " ('taking', 'out'),\n",
       " ('out', 'the'),\n",
       " ('the', 'pins'),\n",
       " ('pins', 'in'),\n",
       " ('in', 'my'),\n",
       " ('my', 'glock'),\n",
       " ('glock', '32'),\n",
       " ('32', 'very'),\n",
       " ('very', 'easy'),\n",
       " ('easy', '\\n'),\n",
       " ('\\n', '<eos>')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_ngram[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a complete list of trigrams occurring in the corpus. Which are the 10 most frequent trigrams?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine count statistics on the trigram frequencies, i.e. compute so-called count-counts (how many trigrams occur once, twice, : : :). Plot their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create N-gram Vocabulary with Corresponding Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 100000\n",
    "\n",
    "all_train_tokens = list(mit.flatten(train_data_ngram + valid_data_ngram))\n",
    "counted_tokens = Counter(all_train_tokens)\n",
    "\n",
    "vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86862, 41952, 41952)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at some numbers!\n",
    "len(all_train_tokens), len(vocab), len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((('\\n', '<eos>'), ('of', 'the')), (900, 332))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:2], count[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create N-gram Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index 1 for unk, 0 for pad, 1 for bos, 2 for eos\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "id2token = list(vocab)\n",
    "token2id = dict(zip(vocab, range(4, 4+len(vocab)))) \n",
    "id2token = ['<pad>', '<unk>', '<bos>', '<eos>'] + id2token\n",
    "\n",
    "token2id['<pad>'] = PAD_IDX \n",
    "token2id['<unk>'] = UNK_IDX\n",
    "token2id['<bos>'] = BOS_IDX \n",
    "token2id['<eos>'] = EOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41956, 41956)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2token), len(token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the dictionary by loading random token from it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 30366 ; token ('little', 'duller')\n",
      "Token ('little', 'duller'); token id 30366\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a list of trigrams in the corpus using only the words in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate count statistics of the trigram frequencies for this modified corpus as well. What do you notice in comparison to the previous exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the out-of-vocabulary (OOV) rate, i.e. the percentage of running words in the corpus which are not covered by the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Converting from Token to ID and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text2id(doc):\n",
    "    return [token2id[t] if t in token2id else UNK_IDX for t in doc]\n",
    "\n",
    "def _id2text(vec):\n",
    "    return [id2token[i] for i in vec]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_id = []\n",
    "for d in train_data_ngram:\n",
    "    train_data_id.append(_text2id(d))\n",
    "    \n",
    "valid_data_id = []\n",
    "for d in valid_data_ngram:\n",
    "    valid_data_id.append(_text2id(d))\n",
    "    \n",
    "train_data_id_merged = []\n",
    "for d in train_data_id:\n",
    "    train_data_id_merged.append((d, 0))\n",
    "\n",
    "valid_data_id_merged = []\n",
    "for d in valid_data_id:\n",
    "    valid_data_id_merged.append((d, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,\n",
       " 800,\n",
       " ([24,\n",
       "   9866,\n",
       "   2508,\n",
       "   3418,\n",
       "   3419,\n",
       "   370,\n",
       "   19,\n",
       "   52,\n",
       "   9867,\n",
       "   9868,\n",
       "   399,\n",
       "   136,\n",
       "   139,\n",
       "   9869,\n",
       "   9870,\n",
       "   435,\n",
       "   5240,\n",
       "   9871,\n",
       "   21,\n",
       "   887,\n",
       "   9872,\n",
       "   9873,\n",
       "   324,\n",
       "   9874,\n",
       "   9875,\n",
       "   31,\n",
       "   1316,\n",
       "   9876,\n",
       "   9877,\n",
       "   702,\n",
       "   9878,\n",
       "   4],\n",
       "  0))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_id), len(train_data_id_merged), train_data_id_merged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " ('<bos>', 'this'),\n",
       " [24,\n",
       "  9866,\n",
       "  2508,\n",
       "  3418,\n",
       "  3419,\n",
       "  370,\n",
       "  19,\n",
       "  52,\n",
       "  9867,\n",
       "  9868,\n",
       "  399,\n",
       "  136,\n",
       "  139,\n",
       "  9869,\n",
       "  9870,\n",
       "  435,\n",
       "  5240,\n",
       "  9871,\n",
       "  21,\n",
       "  887,\n",
       "  9872,\n",
       "  9873,\n",
       "  324,\n",
       "  9874,\n",
       "  9875,\n",
       "  31,\n",
       "  1316,\n",
       "  9876,\n",
       "  9877,\n",
       "  702,\n",
       "  9878,\n",
       "  4],\n",
       " 800,\n",
       " 800)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at what we created\n",
    "train_data_id[0][0], train_data_ngram[0][0], train_data_id_merged[0][0], len(train_data_id_merged), len(train_data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1 you generated a trigram frequency list for a given vocabulary.\n",
    "Determine the list of bigram frequencies from it by summing over the first word position:\n",
    "N(v;w) = N(\u0001; v;w) =\n",
    "X\n",
    "u\n",
    "N(u; v;w)\n",
    "Analogously, recompute the frequencies of unigrams.\n",
    "Now, extract bigrams/unigrams directly from the corpus using your implementation from part 1\n",
    "and compare it to the recomputed versions. What do you notice? How could you \f",
    "x this problem\n",
    "without changing the recomputation method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram counts for already tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counts(data, n):\n",
    "    print(\"data item \", data[0], \"\\n\")\n",
    "    data_pad = pad_sentences(data, n)\n",
    "    print(\"padded item \", data_pad[0], \"\\n\")\n",
    "    ngram_data = find_ngrams(data_pad, n)\n",
    "    print(\"ngram item \", ngram_data[0], \"\\n\")\n",
    "    all_train_tokens = list(mit.flatten(ngram_data))\n",
    "    counted_tokens = Counter(all_train_tokens)\n",
    "    vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "    print(\"vocab item \", vocab[0], \"\\n\")\n",
    "    print(\"count item \", count[0], \"\\n\")\n",
    "    return vocab, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Getting N-gram Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_dict(data, n):\n",
    "    data_pad = pad_sentences(data, n)\n",
    "    ngram_data = find_ngrams(data_pad, n)\n",
    "    \n",
    "    all_train_tokens = list(mit.flatten(ngram_data))\n",
    "    counted_tokens = Counter(all_train_tokens)\n",
    "    vocab, count = zip(*counted_tokens.most_common(max_vocab_size))\n",
    "    \n",
    "    # save index 1 for unk, 0 for pad, 1 for bos, 2 for eos\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    BOS_IDX = 2\n",
    "    EOS_IDX = 3\n",
    "\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(4, 4+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>', '<bos>', '<eos>'] + id2token\n",
    "\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    token2id['<bos>'] = BOS_IDX \n",
    "    token2id['<eos>'] = EOS_IDX\n",
    "    \n",
    "    data_ids = []\n",
    "    for d in ngram_data:\n",
    "        data_ids.extend(_text2id(d))\n",
    "\n",
    "    return id2token, token2id, data_ids, vocab, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'came',\n",
       " 'in',\n",
       " 'on',\n",
       " 'time',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'veru',\n",
       " 'happy',\n",
       " 'with',\n",
       " 'it',\n",
       " 'i',\n",
       " 'haved',\n",
       " 'used',\n",
       " 'it',\n",
       " 'already',\n",
       " 'and',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'taking',\n",
       " 'out',\n",
       " 'the',\n",
       " 'pins',\n",
       " 'in',\n",
       " 'my',\n",
       " 'glock',\n",
       " '32',\n",
       " 'very',\n",
       " 'easy',\n",
       " '\\n']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  ['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n'] \n",
      "\n",
      "padded item  ['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n'] \n",
      "\n",
      "ngram item  [('this',), ('came',), ('in',), ('on',), ('time',), ('and',), ('i',), ('am',), ('veru',), ('happy',), ('with',), ('it',), ('i',), ('haved',), ('used',), ('it',), ('already',), ('and',), ('it',), ('makes',), ('taking',), ('out',), ('the',), ('pins',), ('in',), ('my',), ('glock',), ('32',), ('very',), ('easy',), ('\\n',)] \n",
      "\n",
      "vocab item  ('the',) \n",
      "\n",
      "count item  3752 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((('the',), ('i',)), (3752, 2508))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_unigrams, count_unigrams = ngram_counts(train_data_tokenized, 1)\n",
    "vocab_unigrams[:2], count_unigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  ['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n'] \n",
      "\n",
      "padded item  ['<bos>', 'this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n', '<eos>'] \n",
      "\n",
      "ngram item  [('<bos>', 'this'), ('this', 'came'), ('came', 'in'), ('in', 'on'), ('on', 'time'), ('time', 'and'), ('and', 'i'), ('i', 'am'), ('am', 'veru'), ('veru', 'happy'), ('happy', 'with'), ('with', 'it'), ('it', 'i'), ('i', 'haved'), ('haved', 'used'), ('used', 'it'), ('it', 'already'), ('already', 'and'), ('and', 'it'), ('it', 'makes'), ('makes', 'taking'), ('taking', 'out'), ('out', 'the'), ('the', 'pins'), ('pins', 'in'), ('in', 'my'), ('my', 'glock'), ('glock', '32'), ('32', 'very'), ('very', 'easy'), ('easy', '\\n'), ('\\n', '<eos>')] \n",
      "\n",
      "vocab item  ('\\n', '<eos>') \n",
      "\n",
      "count item  800 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((('\\n', '<eos>'), ('of', 'the')), (800, 299))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigrams, count_bigrams = ngram_counts(train_data_tokenized, 2)\n",
    "vocab_bigrams[:2], count_bigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  ['this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n'] \n",
      "\n",
      "padded item  ['<bos>', '<bos>', 'this', 'came', 'in', 'on', 'time', 'and', 'i', 'am', 'veru', 'happy', 'with', 'it', 'i', 'haved', 'used', 'it', 'already', 'and', 'it', 'makes', 'taking', 'out', 'the', 'pins', 'in', 'my', 'glock', '32', 'very', 'easy', '\\n', '<eos>', '<eos>'] \n",
      "\n",
      "ngram item  [('<bos>', '<bos>', 'this'), ('<bos>', 'this', 'came'), ('this', 'came', 'in'), ('came', 'in', 'on'), ('in', 'on', 'time'), ('on', 'time', 'and'), ('time', 'and', 'i'), ('and', 'i', 'am'), ('i', 'am', 'veru'), ('am', 'veru', 'happy'), ('veru', 'happy', 'with'), ('happy', 'with', 'it'), ('with', 'it', 'i'), ('it', 'i', 'haved'), ('i', 'haved', 'used'), ('haved', 'used', 'it'), ('used', 'it', 'already'), ('it', 'already', 'and'), ('already', 'and', 'it'), ('and', 'it', 'makes'), ('it', 'makes', 'taking'), ('makes', 'taking', 'out'), ('taking', 'out', 'the'), ('out', 'the', 'pins'), ('the', 'pins', 'in'), ('pins', 'in', 'my'), ('in', 'my', 'glock'), ('my', 'glock', '32'), ('glock', '32', 'very'), ('32', 'very', 'easy'), ('very', 'easy', '\\n'), ('easy', '\\n', '<eos>'), ('\\n', '<eos>', '<eos>')] \n",
      "\n",
      "vocab item  ('\\n', '<eos>', '<eos>') \n",
      "\n",
      "count item  800 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((('\\n', '<eos>', '<eos>'), ('<bos>', '<bos>', 'i')), (800, 277))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_trigrams, count_trigrams = ngram_counts(train_data_tokenized, 3)\n",
    "vocab_trigrams[:2], count_trigrams[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the list of bigram frequencies from it by summing over the first word position:\n",
    "\n",
    "#### $ N(u, v, w) = \\sum_u N(\\cdot, v, w) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same for the unigrams. Do we get the same numbers as before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$P(w|w_{−n}, ..., w_{−2}, w_{−1}) \\approx \\frac{c(w_{−n}, ..., w_{−2}, w_{−1}, w)}{\\sum_{w \\in V} c(w_{−n}, ..., w_{−2}, w_{−1}, w)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Probabilities\n",
    "\n",
    "## $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{\\sum_{w_i} c(w_{i-1}, w_i)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram LM\n",
    "##  $$p(s) = \\prod_{i = 1} ^ {N + 1} p(w_i | w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the probability of an n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_ngram(data, token, n):\n",
    "    id2token, token2id, data_ids, vocab, count = ngram_dict(data, n)\n",
    "    if token in token2id:\n",
    "        return np.sum([i == token_id for i in data])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_ngram(data, token, n):\n",
    "    count = get_count_ngram(data, token, n)\n",
    "    all_counts = 0\n",
    "    for t in vocab:\n",
    "        all_counts += get_count_ngram(data, t, n)\n",
    "    if all_counts > 0:\n",
    "        return count / all_counts\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for computing the probability of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-97-26cfecd6b523>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-97-26cfecd6b523>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def get_prob_sentence(vocab, count, n):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "### Bigram LM: $$ p(i \\; love \\; this \\; light) = p(i|\\cdot) \\; p(love|i)\\;  p(this|love)\\;  p(light|this) \\\\\n",
    "\\approx \\frac{c(i, \\cdot)}{\\sum_w c(\\cdot, \\; w)} \\; \\frac{c(love, i)}{\\sum_wc(i, \\; w)}\\;  \\frac{c(this, love)}{\\sum_wc(love, \\;w)}\\;  \\frac{c(light, this)}{\\sum_wc(this, \\;w)}$$ \n",
    "\n",
    "### Trigram LM: $$ p(i \\; love \\; this  \\;light) = p(i|\\cdot, \\cdot) \\; p(love|\\cdot, i) \\; p(this|i, love)\\;  p(light|love, this)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example -- where this approach usually fails\n",
    "\n",
    "### Bigram LM: $$ p(john \\; went \\; to \\; the \\; moon) = p(john|\\cdot) p(went|john) p(to|went) p(the|to) p(moon|the)$$ \n",
    "\n",
    "### Trigram LM: $$ pp(john \\; went \\; to \\; the \\; moon = p(john|\\cdot, \\cdot) p(went|\\cdot, john) p(to|john, went) p(the|went, to) p(moon|to, the)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing -- which ones to show? comparisons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need smoothing for n-gram language modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-One Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing: Linear Interpolation with Absolute Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$p_{bi} = max \\large{ \\frac{N(v, w) - b_{bi}}{N(v), 0}  + b_{bi} \\frac{V - N_0(v, \\cdot)}{N(v)} p_{uni(w)} \\large}$$\n",
    "\n",
    "### $$p_{uni} = max \\large{ \\frac{N(w) - b_{uni}}{N, 0}  + b_{uni} \\frac{V - N_0(\\cdot)}{N} \\frac{1}{V}}$$\n",
    "\n",
    "### $$b_{bi} = \\frac{N_1(\\cdot, \\cdot)}{N_1(\\cdot, \\cdot) + 2*N_2(\\cdot, \\cdot)}$$\n",
    "\n",
    "### $$b_{uni} = \\frac{N_1(\\cdot)}{N_1(\\cdot) + 2*N_2(\\cdot)}$$\n",
    "\n",
    "\n",
    "### $$N_r(\\cdot) = \\sum_{w: N(w) = r} 1$$\n",
    "\n",
    "### $$N_r(\\cdot, \\cdot) = \\sum_{v, w: N(v, w) = r} 1$$\n",
    "\n",
    "### $$N_r(v, \\cdot) = \\sum_{w: N(v, w) = r} 1$$\n",
    "\n",
    "### V is the number of words in the vocabulary\n",
    "\n",
    "### $N_r(\\cdot, \\cdot)$ and $N_r(\\cdot)$  are the count-counts for bigrams and unigrams respectively $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Let's compute the bigram frequencies / probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that the probabilities sum up to one\n",
    "### $$\\sum_w p_{bi}(w|v) = \\sum_w p_{uni}(w) = 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the sums\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at some examples and see if they make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $PP = exp(-\\frac{LL}{\\sum_k(N_k + 1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $LL = \\sum_{k=1}^{K} \\sum_{n=1}^{N_k + 1} log p_{bi}(w_{k,n} | w_{k,n-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentece Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a PyTorch Dataset out of our set of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data_list, max_inp_length=None, device='cpu'):\n",
    "        \"\"\"\n",
    "        data_list is a list of tuples: (x,y) where x is a list of ids and y is a label\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.max_len = max_inp_length\n",
    "        self.data_tensors = []\n",
    "        for (i, t) in tqdm_notebook(self.data):\n",
    "            self.data_tensors.append((torch.LongTensor(i[:self.max_len]), torch.LongTensor([t])))\n",
    "            #TODO: fix error regarding to(device)\n",
    "#             self.data_tensors.append((torch.LongTensor(i[:self.max_len]).to(device), torch.LongTensor([t]).to(device)))\n",
    "              \n",
    "    def __getitem__(self, key):\n",
    "        (inp, tgt) = self.data_tensors[key]\n",
    "        \n",
    "        return inp, tgt, len(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def pad(tensor, length, dim=0, pad=0):\n",
    "    \"\"\"Pad tensor to a specific length.\n",
    "    :param tensor: vector to pad\n",
    "    :param length: new length\n",
    "    :param dim: (default 0) dimension to pad\n",
    "    :returns: padded tensor if the tensor is shorter than length\n",
    "    \"\"\"\n",
    "    if tensor.size(dim) < length:\n",
    "        return torch.cat(\n",
    "            [tensor, tensor.new(*tensor.size()[:dim],\n",
    "                                length - tensor.size(dim),\n",
    "                                *tensor.size()[dim + 1:]).fill_(pad)],\n",
    "            dim=dim)\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def batchify(batch):\n",
    "    maxlen = max(batch, key = itemgetter(2))[-1]\n",
    "    batch_list = []\n",
    "    target_list = []\n",
    "    for b in batch:\n",
    "        batch_list.append(pad(b[0], maxlen, dim=0, pad=PAD_IDX))\n",
    "        target_list.append(b[1])\n",
    "    input_batch = torch.stack(batch_list, 0)\n",
    "    target_batch = torch.stack(target_list, 0)\n",
    "    \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76ceeef98f84820ac684a1483267b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da875a0ef9a24848b6889567afee37e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImdbDataset(train_data_id_merged, max_inp_length=None, device='cuda')\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, collate_fn=batchify, shuffle=True)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data_id_merged, max_inp_length=None, device='cuda')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ATBQ4WaJR93"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(41956, 30, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=30, out_features=2048, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.0)\n",
       "    (3): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BagOfNGrams(nn.Module):\n",
    "    def init_layers(self):\n",
    "        for l in self.layers:\n",
    "            if getattr(l, 'weight', None) is not None:\n",
    "                torch.nn.init.xavier_uniform_(l.weight)\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim=300, hidden_size=512, reduce='sum', nlayers=2, act='ReLU', nclasses=2, dropout=0.1, batch_norm=False):\n",
    "        super(BagOfNGrams, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.reduce = reduce\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.act = getattr(nn, act)\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_dim, mode=reduce)\n",
    "        if batch_norm is True:\n",
    "            self.batch_norm = nn.BatchNorm1d(self.emb_dim)\n",
    "        self.layers = nn.ModuleList([nn.Linear(self.emb_dim, self.hidden_size)])\n",
    "        self.layers.append(self.act())\n",
    "        self.layers.append(nn.Dropout(p=dropout))\n",
    "        for i in range(self.nlayers-2):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(self.act())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(self.hidden_size, 1))\n",
    "        self.init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        postemb = self.embedding(x)\n",
    "        if hasattr(self, 'batch_norm'):\n",
    "            x = self.batch_norm(postemb)\n",
    "        else:\n",
    "            x = postemb\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BagOfNGrams(len(id2token), emb_dim=30, hidden_size=2048, act='Tanh', nlayers=1, reduce='mean', dropout=0.0, batch_norm=False)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, nesterov=True)\n",
    "#optimizer = torch.optim.Adagrad(params=model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, labels in loader:\n",
    "        outputs = torch.sigmoid(model(data))\n",
    "        predicted = (outputs > 0.5).long()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxfi7ruGHZu3"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.18it/s]\n",
      "2it [00:00,  5.23it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.35it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.15it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.59it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.55it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.82it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  4.70it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.80it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.01it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.92it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  4.47it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.62it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.65it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.98it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.31it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/20], Step: [2/2], Train loss: 0.0, Validation Acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO fix cuda issue!!! \n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in tqdm(enumerate(train_loader)): \n",
    "        data.cpu()\n",
    "        labels.cpu()\n",
    "        model.cpu()\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        outputs.cuda()\n",
    "        loss = criterion(outputs.view(-1), labels.float().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    if epoch % 1 == 0 and epoch > 0:\n",
    "        val_acc = test_model(loader=valid_loader, model=model)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Train loss: {}, Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss.item(), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v2+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v2.6.0.json",
       "config": {
        "view": {
         "height": 300,
         "width": 400
        }
       },
       "data": {
        "name": "data-f2154aae4754ceff2c863deff6d5ca74"
       },
       "datasets": {
        "data-f2154aae4754ceff2c863deff6d5ca74": [
         {
          "loss": 0.6279397010803223,
          "step": 0
         },
         {
          "loss": 0,
          "step": 1
         },
         {
          "loss": 0,
          "step": 2
         },
         {
          "loss": 0,
          "step": 3
         },
         {
          "loss": 0,
          "step": 4
         },
         {
          "loss": 0,
          "step": 5
         },
         {
          "loss": 0,
          "step": 6
         },
         {
          "loss": 0,
          "step": 7
         },
         {
          "loss": 0,
          "step": 8
         },
         {
          "loss": 0,
          "step": 9
         },
         {
          "loss": 0,
          "step": 10
         },
         {
          "loss": 0,
          "step": 11
         },
         {
          "loss": 0,
          "step": 12
         },
         {
          "loss": 0,
          "step": 13
         },
         {
          "loss": 0,
          "step": 14
         },
         {
          "loss": 0,
          "step": 15
         },
         {
          "loss": 0,
          "step": 16
         },
         {
          "loss": 0,
          "step": 17
         },
         {
          "loss": 0,
          "step": 18
         },
         {
          "loss": 0,
          "step": 19
         },
         {
          "loss": 0,
          "step": 20
         },
         {
          "loss": 0,
          "step": 21
         },
         {
          "loss": 0,
          "step": 22
         },
         {
          "loss": 0,
          "step": 23
         },
         {
          "loss": 0,
          "step": 24
         },
         {
          "loss": 0,
          "step": 25
         },
         {
          "loss": 0,
          "step": 26
         },
         {
          "loss": 0,
          "step": 27
         },
         {
          "loss": 0,
          "step": 28
         },
         {
          "loss": 0,
          "step": 29
         },
         {
          "loss": 0,
          "step": 30
         },
         {
          "loss": 0,
          "step": 31
         },
         {
          "loss": 0,
          "step": 32
         },
         {
          "loss": 0,
          "step": 33
         },
         {
          "loss": 0,
          "step": 34
         },
         {
          "loss": 0,
          "step": 35
         },
         {
          "loss": 0,
          "step": 36
         },
         {
          "loss": 0,
          "step": 37
         },
         {
          "loss": 0,
          "step": 38
         },
         {
          "loss": 0,
          "step": 39
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "step",
         "scale": {},
         "type": "quantitative"
        },
        "y": {
         "field": "loss",
         "scale": {
          "type": "log"
         },
         "type": "quantitative"
        }
       },
       "height": 400,
       "mark": "line",
       "width": 800
      },
      "image/png": "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",
      "text/plain": [
       "<VegaLite 2 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install altair\n",
    "import altair as alt\n",
    "import pandas\n",
    "\n",
    "batch_loss = []\n",
    "for i,l in enumerate(train_losses):\n",
    "    batch_loss.append((i,l))\n",
    "\n",
    "df = pandas.DataFrame(batch_loss, columns=['step', 'loss'])\n",
    "\n",
    "alt.Chart(df).mark_line().encode(\n",
    "    alt.X('step', scale=alt.Scale()),\n",
    "    alt.Y('loss', scale=alt.Scale(type='log'))).properties(\n",
    "        width=800,\n",
    "        height=400\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhi0Ww8JKEkk"
   },
   "source": [
    "## Analysis & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare N-gram LM to Neural LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec:  19%|█▉        | 1.25G/6.60G [11:49<50:35, 1.76MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3abfa44c94ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.vector_cache'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url)\u001b[0m\n\u001b[1;32m    261\u001b[0m                             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# remove the partial zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting vectors into {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = vocab.FastText(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(dataset, max_size=30000, vectors=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a batch iterator\n",
    "train_loader = data.BucketIterator(dataset=dataset, batch_size=4, sort_key=lambda x: len(x.reviewText), device=torch.device('cpu'), sort_within_batch=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vec2txt(vec):\n",
    "    return [text_field.vocab.itos[t] for t in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.reviewText[0][0])\n",
    "print(_vec2txt(batch.reviewText[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jzyCRhtBNit"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFDnznsFJRMI"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "ngram-lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
