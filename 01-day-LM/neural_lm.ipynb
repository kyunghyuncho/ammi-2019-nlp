{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyunghyuncho/ammi-2019-nlp/blob/master/01-day-LM/neural_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNniJxhFMYCF"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('utils/')\n",
    "import loading_text_and_tokenization\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.ngram_utils as ngram_utils\n",
    "from utils.ngram_utils import NgramLM\n",
    "from utils.amazon_dataset import AmazonDataset, pad, batchify\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.neural_lm import BagOfNGrams, DecoderMLP, seq2seq\n",
    "import utils.global_variables as gl\n",
    "import torch\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "_tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1b080207b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_cuda) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .txt files and create lists of reviews\n",
    "train_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_train.txt', 'r') as f:\n",
    "    train_data = [review for review in f.read().split('\\n') if review]\n",
    "    \n",
    "valid_data = []\n",
    "# create a list of all the reviews \n",
    "with open('../data/amazon_reviews_clothing_valid.txt', 'r') as f:\n",
    "    valid_data = [review for review in f.read().split('\\n') if review]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = split_into_sentences(train_data)\n",
    "# valid_data = split_into_sentences(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222919, 27869)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"this is a great tutu and at a really great price . it doesn ' t look cheap at all . i ' m so glad i looked on amazon and found such an affordable tutu that isn ' t made poorly . a + + \",\n",
       " list,\n",
       " 100,\n",
       " str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], valid_data[0]\n",
    "train_data = train_data[:100]\n",
    "valid_data = valid_data[:10]\n",
    "train_data[0], type(train_data), len(train_data), type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd98a3c1989432cbdb1bd41a98cbaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5412fb6f724dc49087f18d13fb7ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Datasets\n",
    "# TODO: this takes a really long time !! why?\n",
    "train_data_tokenized, all_tokens_train = ngram_utils.tokenize_dataset(train_data)\n",
    "valid_data_tokenized, all_tokens_valid = ngram_utils.tokenize_dataset(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'great',\n",
       "  'tutu',\n",
       "  'and',\n",
       "  'at',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'price',\n",
       "  '.'],\n",
       " 'this')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0], all_tokens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_padded = ngram_utils.pad_dataset(train_data_tokenized, n=N)\n",
    "valid_data_padded = ngram_utils.pad_dataset(valid_data_tokenized, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " '<sos>',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'tutu',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'really',\n",
       " 'great',\n",
       " 'price',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, ('<sos>', '<eos>', '.', ',', 'the', 'i', 'to', 'and', 'a', 'it'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ngram_utils.get_vocab(train_data_padded)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size, vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token, token2id = ngram_utils.get_dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ids = ngram_utils.get_ids(train_data_padded, token2id)\n",
    "valid_data_ids = ngram_utils.get_ids(valid_data_padded, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 844/844 [00:03<00:00, 278.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AmazonDataset(train_data_ids, max_inp_length=None, use_cuda=True)\n",
    "train_dataset_ngrams = []\n",
    "for t in train_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        train_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "train_loader = DataLoader(train_dataset_ngrams, batch_size=1024, collate_fn=batchify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in train_dataset_ngrams:\n",
    "#     print(t)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (d, l) in enumerate(train_loader):\n",
    "#     import pdb; pdb.set_trace()\n",
    "#     print(d)\n",
    "#     print(l)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:00<00:00, 14661.46it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = AmazonDataset(valid_data_ids, max_inp_length=None, use_cuda=True)\n",
    "valid_dataset_ngrams = []\n",
    "for t in valid_dataset:\n",
    "    for i in range(len(t) - N):\n",
    "        valid_dataset_ngrams.append((t[i:i + N], t[i + N]))\n",
    "valid_loader = DataLoader(valid_dataset_ngrams, batch_size=1024, collate_fn=batchify, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f1a32aa4b38>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18235, 659)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(train_dataset_ngrams)\n",
    "num_valid = len(valid_dataset_ngrams)\n",
    "num_train, num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfNGrams(\n",
       "  (embedding): EmbeddingBag(1984, 300, mode=mean)\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = BagOfNGrams(len(id2token), emb_dim=300, hidden_size=512, out_size=256, activation='ReLU', nlayers=1, reduce='mean', dropout=0.1, batch_norm=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderMLP(\n",
       "  (linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (out): Linear(in_features=512, out_features=1984, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderMLP(input_size=256, output_size=len(id2token), hidden_size=512)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): BagOfNGrams(\n",
       "    (embedding): EmbeddingBag(1984, 300, mode=mean)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1)\n",
       "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderMLP(\n",
       "    (linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (out): Linear(in_features=512, out_features=1984, bias=True)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq2seq(encoder, decoder, id2token, use_cuda=False, lr=0.1, size_ngrams=N)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 | Train Loss  0.01 | Train PPL     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   0 | Valid Loss  0.01 | Valid PPL     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Train Loss  0.01 | Train PPL     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   1 | Valid Loss  0.01 | Valid PPL     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Train Loss  0.00 | Train PPL     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| Epoch   2 | Valid Loss  0.01 | Valid PPL     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "log_interval = 1\n",
    "best_eval_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = 0        \n",
    "    for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "        prediction, loss = model.train_step(data, labels)\n",
    "        train_loss += loss\n",
    "    train_loss = train_loss / num_train\n",
    "    print('| Epoch {:3d} | Train Loss {:5.2f} | Train PPL {:8.2f}'.format(\n",
    "            epoch, train_loss, math.exp(train_loss)))\n",
    "\n",
    "    # Eval\n",
    "    if epoch % log_interval == 0:        \n",
    "        eval_loss = 0\n",
    "        for i, (data, labels) in _tqdm(enumerate(valid_loader), disable=True):\n",
    "            prediction, loss = model.train_step(data, labels, eval_mode=True)\n",
    "            eval_loss += loss\n",
    "        eval_loss = eval_loss / num_valid\n",
    "        print('-' * 89)\n",
    "        print('| Epoch {:3d} | Valid Loss {:5.2f} | Valid PPL {:8.2f}'.format(\n",
    "            epoch, eval_loss, math.exp(eval_loss)))\n",
    "        print('-' * 89)\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_eval_loss or eval_loss < best_eval_loss:\n",
    "            with open('neural_lm_amazon_model' + '.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_eval_loss = eval_loss        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sent):\n",
    "    import pdb; pdb.set_trace()\n",
    "    tokenized, _ = ngram_utils.tokenize_dataset(sent)\n",
    "    sent_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "    sent_tensor = torch.LongTensor(sent_ids).to(device)\n",
    "    generated, scores = model.eval_step(sent_tensor, score_only=True)\n",
    "    ppl = math.exp(scores)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-32-6f4884bc8cbd>(3)score_sentence()\n",
      "-> tokenized, _ = ngram_utils.tokenize_dataset(sent)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133877fc1d32499aae6aaf6ba4bc320b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(156)eval_step()\n",
      "-> if xs is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(167)eval_step()\n",
      "-> if score_only or not use_context:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(214)eval_step()\n",
      "-> predictions = [self.v2t(p) for p in predictions]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([18]), tensor([16]), tensor([12]), tensor([63]), tensor([191])]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  predictions.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  len(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0456609491681853"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['this is a great tutu']\n",
    "type(sent), len(sent), type(sent[0]), sent[0]\n",
    "ppl = score_sentence(sent)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(context=None):\n",
    "    import pdb; pdb.set_trace()\n",
    "    if context is None:\n",
    "        dummy_context = torch.LongTensor([[0]]).to(device)\n",
    "        generated, scores = model.eval_step(dummy_context, use_context=False)\n",
    "    else:\n",
    "        tokenized, _ = ngram_utils.tokenize_dataset(context)\n",
    "        context_ids = ngram_utils.get_ids(tokenized, token2id)\n",
    "        context_tensor = torch.LongTensor(context_ids).to(device)\n",
    "        generated, scores = model.eval_step(context_tensor, use_context=True)\n",
    "    \n",
    "    ppl = math.exp(scores)\n",
    "    return generated, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-137dbbbf69b1>(3)generate_sentence()\n",
      "-> if context is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(156)eval_step()\n",
      "-> if xs is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(167)eval_step()\n",
      "-> if score_only or not use_context:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(214)eval_step()\n",
      "-> predictions = [self.v2t(p) for p in predictions]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  len(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  predictions[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',', \"'\", \"'\", \"'\", \"'\", 't', 't', 't', '.', '<eos>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated, scores = generate_sentence()\n",
    "generated_str = [' '.join(g) for g in generated]\n",
    "generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-137dbbbf69b1>(3)generate_sentence()\n",
      "-> if context is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-137dbbbf69b1>(7)generate_sentence()\n",
      "-> tokenized, _ = ngram_utils.tokenize_dataset(context)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a great tutu']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3969d0671db244e18d8bb339ac69897d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> <ipython-input-38-137dbbbf69b1>(8)generate_sentence()\n",
      "-> context_ids = ngram_utils.get_ids(tokenized, token2id)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-137dbbbf69b1>(9)generate_sentence()\n",
      "-> context_tensor = torch.LongTensor(context_ids).to(device)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  context_ids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 16, 12, 63, 191]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-137dbbbf69b1>(10)generate_sentence()\n",
      "-> generated, scores = model.eval_step(context_tensor, use_context=True)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  context_tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 18,  16,  12,  63, 191]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(156)eval_step()\n",
      "-> if xs is None:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(158)eval_step()\n",
      "-> xs = xs.to(self.device)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(159)eval_step()\n",
      "-> bsz = xs.size(0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(162)eval_step()\n",
      "-> self.encoder.eval()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(163)eval_step()\n",
      "-> self.decoder.eval()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(165)eval_step()\n",
      "-> import pdb; pdb.set_trace()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(167)eval_step()\n",
      "-> if score_only or not use_context:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(171)eval_step()\n",
      "-> encoder_input = xs   # this needs to be of shape bsz, self.size_ngrams\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(173)eval_step()\n",
      "-> predictions = []\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  encoder_input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 18,  16,  12,  63, 191]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/roberta/ammi-2019-nlp/01-day-LM/utils/neural_lm.py(214)eval_step()\n",
      "-> predictions = [self.v2t(p) for p in predictions]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.', '<eos>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated, scores = generate_sentence(context=['this is a great tutu'])\n",
    "generated_str = [' '.join(g) for g in generated]\n",
    "generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_context = True\n",
    "# score_only = True\n",
    "# K = 5\n",
    "# for i, (data, labels) in _tqdm(enumerate(valid_loader), disable=True):\n",
    "#     import pdb; pdb.set_trace()\n",
    "#     generated, scores = model.eval_step(data, use_context=use_context, score_only=score_only)            # batch predictions\n",
    "#     for k in range(K):\n",
    "#         if use_context:\n",
    "#             context = [model.v2t(d) for d in data][k]\n",
    "#             context = [c[0] for c in context]\n",
    "#             print(\"Context: \", ' '.join(context))  # print only one generated sentence out of the bsz \n",
    "#         generated_str = [' '.join(g) for g in generated] # convert them to more readable strings     \n",
    "#         if not score_only:\n",
    "#             print(\"Generated \", generated_str[k])  # print only one generated sentence out of the bsz \n",
    "#         print(\"Score:    \", math.exp(scores[k]))  # print only one generated sentence out of the bsz \n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_context = False\n",
    "# score_only = False\n",
    "# for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "#     generated, scores = model.eval_step(data, use_context=use_context, score_only=score_only)            # batch predictions\n",
    "#     import pdb; pdb.set_trace()\n",
    "#     for k in range(K):\n",
    "#         if use_context:\n",
    "#             context = [model.v2t(d) for d in data][0]\n",
    "#             context = [c[0] for c in context]\n",
    "#             print(\"Context: \", ' '.join(context))  # print only one generated sentence out of the bsz \n",
    "#         generated_str = [' '.join(g) for g in generated] # convert them to more readable strings     \n",
    "#         print(\"Generated \", generated_str[0])  # print only one generated sentence out of the bsz \n",
    "#         print(\"Score:    \", math.exp(scores[0]))  # print only one generated sentence out of the bsz \n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_context = True\n",
    "# score_only = False\n",
    "# for i, (data, labels) in _tqdm(enumerate(train_loader), disable=True):\n",
    "#     generated, scores = model.eval_step(data, use_context=use_context, score_only=score_only)            # batch predictions\n",
    "\n",
    "#     for k in range(K):\n",
    "#         if use_context:\n",
    "#             context = [model.v2t(d) for d in data][k]\n",
    "#             context = [c[0] for c in context]\n",
    "#             print(\"Context: \", ' '.join(context))  # print only one generated sentence out of the bsz \n",
    "#         generated_str = [' '.join(g) for g in generated] # convert them to more readable strings     \n",
    "#         print(\"Generated \", generated_str[k])  # print only one generated sentence out of the bsz \n",
    "#         print(\"Score:    \", math.exp(scores[k]))  # print only one generated sentence out of the bsz \n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "neural_lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
